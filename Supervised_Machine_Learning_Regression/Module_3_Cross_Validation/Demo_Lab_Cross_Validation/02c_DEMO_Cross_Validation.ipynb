{"cells":[{"cell_type":"markdown","id":"12ec4e02-9c8f-4830-9d88-5e5f11538165","metadata":{},"source":["# Machine Learning Foundation\n","\n","## Section 2, Part c: Cross Validation \n"]},{"cell_type":"markdown","id":"4508ca30-2138-4049-a016-51d851750f78","metadata":{},"source":["## Learning objectives\n","\n","By the end of this lesson, you will be able to:\n","\n","* Chain multiple data processing steps together using `Pipeline`\n","* Use the `KFolds` object to split data into multiple folds.\n","* Perform cross validation using SciKit Learn with `cross_val_predict` and `GridSearchCV`\n","\n","To download the data set (.pickle file) click [HERE](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle)\n","\n","\n","Welcome to our third notebook here in the second course on cross-validation. In this notebook, we will discuss how to chain multiple data processing steps together using the pipeline functionality, which will allow you to speed up a lot of your Machine Learning workflow. We'll talk about using the KFolds object to split data into multiple folds as we saw in lecture, and then we'll learn how to perform cross-validation using sklearn's cross_val_predict, as well as the GridSearchCV to see how well we are able to perform given our model on each one of these folds. Now, as usual, we're going to bring in many different libraries. Let's go down to the second line here on sklearn. From model selection, we're going to bring in the objects we just mentioned, KFold and cross_val_predict. From the linear model, we're not just going to bring in linear regression, but we're also going to introduce lasso and ridge regression. We're going to talk more about these in detail during lecture, but note that these are just linear regression plus a way of ensuring that you don't overfit that linear regression. Then from metrics, we're bringing in the r2_score, which is just r squared, and then from pipeline, we're bringing in the pipeline function."]},{"cell_type":"code","execution_count":1,"id":"fb6b84a6-ef92-461b-8501-e9fdcb886889","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (2.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: pandas in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2.9.0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: matplotlib in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (3.9.2)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.23 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (2.0.1)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=8 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (2.9.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: scikit-learn in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (1.5.1)\n","Requirement already satisfied: numpy>=1.19.5 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (2.0.1)\n","Requirement already satisfied: scipy>=1.6.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (1.14.0)\n","Requirement already satisfied: joblib>=1.2.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (3.5.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: wget in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (3.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Surpress warnings:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","%pip install numpy\n","%pip install pandas\n","%pip install matplotlib\n","%pip install scikit-learn\n","%pip install wget\n","import wget\n","import numpy as np\n","import pickle\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.model_selection import KFold, cross_val_predict\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge\n","from sklearn.metrics import r2_score\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"markdown","metadata":{},"source":["Now our dataset this time is going to be saved in a pickle file. This pickle file is actually a dictionary. Pickle allows us to save Python objects and be able to achieve them easily. So we're going to open up this pickle file that we have here. If we look at boston.keys, we see this is a dictionary with the keys dataframe and description, and we're going to pull out the pandas DataFrame specifically, call that boston_data, and then we're also going to plot the boston description to separate these two out into their own objects. Then we see we have the pandas DataFrame and we can look at the first five rows and see the median value is what we're trying to predict for the housing and we also have all these different features in order to help us predict that median value."]},{"cell_type":"code","execution_count":2,"id":"e74895c8-2fe0-461a-bb48-16604b6d831a","metadata":{},"outputs":[],"source":["## Note we are loading a slightly different (\"cleaned\") pickle file\n","# !wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle\n","# boston = pickle.load(open('data/boston_housing_clean.pickle', \"rb\" ))\n","# boston = pickle.load(open('boston_housing_clean.pickle', \"rb\" ))"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle\"\n","downloaded_file = wget.download(url)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["with open(downloaded_file, 'rb') as to_read:\n","    boston = pd.read_pickle(to_read)"]},{"cell_type":"code","execution_count":5,"id":"54e98d31-1702-4e5e-8792-cff1719362ea","metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['dataframe', 'description'])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["boston.keys()"]},{"cell_type":"code","execution_count":6,"id":"87cacd66-5eb7-4ef0-9586-5c0c9b6eb7dc","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.11.9\n"]}],"source":["!python --version"]},{"cell_type":"code","execution_count":7,"id":"d852887b-25a7-407e-9d30-2c27c97cbfeb","metadata":{},"outputs":[],"source":["boston_data = boston['dataframe']\n","boston_description = boston['description']"]},{"cell_type":"code","execution_count":8,"id":"e4c71176-ec0e-46c6-9151-e3722d875692","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CRIM</th>\n","      <th>ZN</th>\n","      <th>INDUS</th>\n","      <th>CHAS</th>\n","      <th>NOX</th>\n","      <th>RM</th>\n","      <th>AGE</th>\n","      <th>DIS</th>\n","      <th>RAD</th>\n","      <th>TAX</th>\n","      <th>PTRATIO</th>\n","      <th>B</th>\n","      <th>LSTAT</th>\n","      <th>MEDV</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00632</td>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0.0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1.0</td>\n","      <td>296.0</td>\n","      <td>15.3</td>\n","      <td>396.90</td>\n","      <td>4.98</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02731</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0.0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2.0</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>396.90</td>\n","      <td>9.14</td>\n","      <td>21.6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.02729</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0.0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2.0</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>392.83</td>\n","      <td>4.03</td>\n","      <td>34.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03237</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0.0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3.0</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>394.63</td>\n","      <td>2.94</td>\n","      <td>33.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.06905</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0.0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3.0</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>396.90</td>\n","      <td>5.33</td>\n","      <td>36.2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n","0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n","1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n","2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n","3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n","4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n","\n","   PTRATIO       B  LSTAT  MEDV  \n","0     15.3  396.90   4.98  24.0  \n","1     17.8  396.90   9.14  21.6  \n","2     17.8  392.83   4.03  34.7  \n","3     18.7  394.63   2.94  33.4  \n","4     18.7  396.90   5.33  36.2  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["boston_data.head()"]},{"cell_type":"markdown","id":"dd1b3bc6-439e-475c-8d20-4a834f1e4781","metadata":{},"source":["### Discussion: \n","\n","Suppose we want to do Linear Regression on our dataset to get an estimate, based on mean squared error, of how well our model will perform on data outside our dataset. \n","\n","Suppose also that our data is split into three folds: Fold 1, Fold 2, and Fold 3.\n","\n","What would the steps be, in English, to do this?\n","\n","Now we know, given our discussion and the subject of this current notebook, that our goal is going to be, how can we predict future values when we only have the data available to us in this dataset? So what we're going to want to do is use KFolds and separate out into three different folds, three different train and test sets, and we want to think about how are we going to do this in Python code."]},{"cell_type":"markdown","id":"93544255-18bf-4186-a647-a3a6267546c9","metadata":{},"source":["**Your response below**\n"]},{"cell_type":"markdown","id":"dd8cb8f1-4f7f-49b8-b884-0442b21c9c03","metadata":{},"source":[" \n"]},{"cell_type":"markdown","id":"8c1e18c0-ce68-4841-b961-cffcb83b80e3","metadata":{},"source":["#### Coding this up\n","\n","So to code this up, the first thing that we want to do is separate out our x and y variables, our features x, and our target variable y. So x is just going to be equal to boston_data, and we're doing.drop just to remove the outcome variable, and then y is just going to be equal to the outcome variable.\n","\n","The [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) object in SciKit Learn tells the cross validation object (see below) how to split up the data:\n"]},{"cell_type":"code","execution_count":9,"id":"b414454e-b4b4-4c15-822e-7de7e601858e","metadata":{},"outputs":[],"source":["X = boston_data.drop('MEDV', axis=1)\n","y = boston_data.MEDV"]},{"cell_type":"markdown","metadata":{},"source":["Then as we've done so far with sklearn, we're going to want to initiate an object. So we're going to initiate our KFold object before using it, and we're going to pass into that initiated object certain arguments. When we say shuffle equals true, if you recall when we were looking at a DataFrame, we can take the first 10 and leave, let's say it was DataFrame of 100 values, we can take the first 10 and leave out the next 90, and then take the next 10. So we started off with zero to nine, and then 10-19, and each one of those can be the different test sets, or we can shuffle it and choose a random 10 to be our test set, the rest being our training set, and then take another random 10. This will become clear as we actually look at the indices that we're pulling out in the next couple of cells. So we're going to shuffle that, so it's not just an order, and then we set our random state, and then the other important argument that we need to pass is the number of splits. So how many times do we want to split up our DataFrame? We're going to have three folds, meaning three training sets and three test sets, where none of the test sets are going to overlap. If you recall from lecture, the training sets can overlap, but the test sets have to be exclusive so that we are looking at different test sets every single time."]},{"cell_type":"code","execution_count":10,"id":"cbe069fd-4f44-4600-9e36-bcec2a9a3da7","metadata":{},"outputs":[],"source":["kf = KFold(shuffle=True, random_state=72018, n_splits=3)"]},{"cell_type":"markdown","metadata":{},"source":["Let's actually look at what kf.split does for us. That's going to give us a generator object. That generator object you can think of as a list. A list where each value in that list is a tuple. That tuple is going to be first, all the indices that we want to set to our train index, and the second part of that tuple is going to be the test index. So it's going to be three tuples, each one with a train index, giving all of the indices taken from our DataFrame, taken from our x, so it's going to be the same size, and then our test index, which is just going to be again, the indices just specifying the test Index. We have our train index, that's just going to be a list of numbers, and that list of numbers is going to be somewhere between, let's just look at x.shape, it's going to be within 506. So values between zero and 505, all the way up until the end."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["<generator object _BaseKFold.split at 0x0000028ABAD10CA0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["kf.split(X)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["(506, 13)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["X.shape"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CRIM</th>\n","      <th>ZN</th>\n","      <th>INDUS</th>\n","      <th>CHAS</th>\n","      <th>NOX</th>\n","      <th>RM</th>\n","      <th>AGE</th>\n","      <th>DIS</th>\n","      <th>RAD</th>\n","      <th>TAX</th>\n","      <th>PTRATIO</th>\n","      <th>B</th>\n","      <th>LSTAT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>501</th>\n","      <td>0.06263</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0.0</td>\n","      <td>0.573</td>\n","      <td>6.593</td>\n","      <td>69.1</td>\n","      <td>2.4786</td>\n","      <td>1.0</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>391.99</td>\n","      <td>9.67</td>\n","    </tr>\n","    <tr>\n","      <th>502</th>\n","      <td>0.04527</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0.0</td>\n","      <td>0.573</td>\n","      <td>6.120</td>\n","      <td>76.7</td>\n","      <td>2.2875</td>\n","      <td>1.0</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>9.08</td>\n","    </tr>\n","    <tr>\n","      <th>503</th>\n","      <td>0.06076</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0.0</td>\n","      <td>0.573</td>\n","      <td>6.976</td>\n","      <td>91.0</td>\n","      <td>2.1675</td>\n","      <td>1.0</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>5.64</td>\n","    </tr>\n","    <tr>\n","      <th>504</th>\n","      <td>0.10959</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0.0</td>\n","      <td>0.573</td>\n","      <td>6.794</td>\n","      <td>89.3</td>\n","      <td>2.3889</td>\n","      <td>1.0</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>393.45</td>\n","      <td>6.48</td>\n","    </tr>\n","    <tr>\n","      <th>505</th>\n","      <td>0.04741</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0.0</td>\n","      <td>0.573</td>\n","      <td>6.030</td>\n","      <td>80.8</td>\n","      <td>2.5050</td>\n","      <td>1.0</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>7.88</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n","501  0.06263  0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n","502  0.04527  0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n","503  0.06076  0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n","504  0.10959  0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n","505  0.04741  0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n","\n","     PTRATIO       B  LSTAT  \n","501     21.0  391.99   9.67  \n","502     21.0  396.90   9.08  \n","503     21.0  396.90   5.64  \n","504     21.0  393.45   6.48  \n","505     21.0  396.90   7.88  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["X.tail()"]},{"cell_type":"markdown","metadata":{},"source":["We're going to look at the first 10 values for each one of our train and test splits. That's the first 10 here (`[:10]`)and then we're also going to get the length of our train index and the length of our test index.\n","\n","Now, think about what the length of our train and test indices should be. If we're splitting into three, then we know that our tests indices should be around one-third of the length of our entire data set. I'm going to run this and we see that 170, which is around one-third of 505, is going to be our tests index size, and it's going to be the same for each one of these folds. One of them is one-half, just because it didn't cleanly divide by 3. But we see that each one of these lengths for test index is going to be around one-third, and then the remaining two-thirds are going to be the size of our training index. All the values that we're going to eventually want to train our model on. We also see that we have the actual indices, just the first 10, here for the train index. Remember, the train indices can overlap, and you should see some overlap. Here, you see the two and the two, two and two, and there should be some overlap, but the tests indices will not have any overlap. These are each going to be unique and these are going to be different holdout sets. We're going to train on this training set and then test on this test set. Then we're going to train on this train set and test on this test set, and so on."]},{"cell_type":"code","execution_count":14,"id":"342508b2-a620-4b0f-9632-738b9a959352","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train index: [ 1  3  4  5  7  8 10 11 12 13] 337\n","Test index: [ 0  2  6  9 15 17 19 23 25 26] 169\n","\n","Train index: [ 0  2  6  9 10 11 12 13 15 17] 337\n","Test index: [ 1  3  4  5  7  8 14 16 22 27] 169\n","\n","Train index: [0 1 2 3 4 5 6 7 8 9] 338\n","Test index: [10 11 12 13 18 20 21 24 28 31] 168\n","\n"]}],"source":["for train_index, test_index in kf.split(X):\n","    print(\"Train index:\", train_index[:10], len(train_index))\n","    print(\"Test index:\",test_index[:10], len(test_index))\n","    print('')"]},{"cell_type":"markdown","metadata":{},"source":["Now, we want to to the scores for each one of our train and test splits. We're going to start off with a blank list for scores. We're going to initiate our linear regression object. Now, we have our predictor, it's lr, and we're going to say for train index and tests index in our split. Again, the split was defined earlier and we saw what that output will look like. That'll be a train index and a test index will run through that, and then we'll get to the next tuple of train and test indices. We're going to set our x_train, x_test, y_train, and y_test to the following outputs. It's going to be x, so our original x with all of our rows, but we're only going to take the rows for our first train index, and we're obviously going to take all of our columns from x. We're then going to set to x_test all of our test indices. So now we've set the x_train and the x_test. Then y_train is just going to be y and it's going to be the matching train index to our x_train that we specified first, and then we're going to set our y towards the test index to match up with the x_test index, which we defined here with x_test.\n","\n","We're then going to fit our model to our training set. Again, this is going to be a for loop. So it's going to do this three times. Using our first training set, we're going to do lr.fit on x_train and y_train. We're going to come up with our predictions on x_test. Again, we fit to our training model and then we test what the predictions will be, assuming we didn't have the labels for x_test, we get our prediction. Then we can say for that prediction, how well did we do according to the actual values that we held out in our test set. So you get the r2_score of y_test.values and y_pread. So r2_score we brought in earlier, recall that was one of the metrics that we pulled in. We set that equal to our score and then we add that onto our original list that we had empty at first. We're going to do this for three different splits. We'll run this code and you see that it will output three different results for each one of those test and training sets, and this also makes clear how you can end up with fairly different values depending on what your test set is. This highlights the importance of doing multiple folds, and then eventually, if you're doing cross validation, you would end up averaging these all together."]},{"cell_type":"code","execution_count":15,"id":"b943e6fe-c046-48f1-b66e-af350eabc77e","metadata":{},"outputs":[{"data":{"text/plain":["[0.6719348798472737, 0.7485020059212378, 0.6976807323597766]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["#from sklearn.metrics import r2_score, mean_squared_error\n","\n","scores = []\n","lr = LinearRegression()\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n","                                        X.iloc[test_index, :], \n","                                        y[train_index], \n","                                        y[test_index])\n","    \n","    lr.fit(X_train, y_train)\n","        \n","    y_pred = lr.predict(X_test)\n","\n","    score = r2_score(y_test.values, y_pred)\n","    \n","    scores.append(score)\n","    \n","scores"]},{"cell_type":"markdown","id":"5170a4a4-ae31-4aaf-b875-6faacd355d6a","metadata":{},"source":["A bit cumbersome, but do-able.\n"]},{"cell_type":"markdown","id":"eafb8410-cddc-499f-8089-cc66d4e8b0c8","metadata":{},"source":["### Discussion (Part 2): \n","\n","Now suppose we want to do the same, but appropriately scaling our data as we go through the folds.\n","\n","What would the steps be _now_?\n"]},{"cell_type":"markdown","id":"b4e2bfc3-4422-4261-a343-b96619c4d20a","metadata":{},"source":["**Your response below**\n"]},{"cell_type":"markdown","metadata":{},"source":["Here we're going to discuss adding in a preprocessing step, as well as introducing the pipeline and the cross val predict, which we'll see will consolidate a lot of the code that we're writing out in this next step."]},{"cell_type":"markdown","id":"d2704c55-e505-41fa-be0c-55d37e9acf1e","metadata":{},"source":["### Coding this up\n","\n","Similar to before, we're going to start off with just an empty lists for scores. We're going to initiate our linear regression object to lr, and then we're also going to add on this preprocessing step of scaling our data. That's going to be this s equals StandardScaler. We're then going to run that same for-loop using our kf.split, and end up with an x_train, x_test, y_train, and a y_test set. We're then going to take our x_train and then fit and transform it using our StandardScalar. We're going to come up with that mean and standard deviation, and subtract the mean, divide by the standard deviation, and come up with this new x_train_s. We've now scaled our data. We're then going to pass in our scale data along with our original outcome variable just for the training set, and we're going to fit our linear regression model on our training set. We then have to transform our test set, getting it to the same scale using what we learned from the standard scalar on the training set. So we just run s.transform on our test set, we get the output when we subtract the mean and the standard deviation of x_test_s, we can then pass that into our linear regression model that's fitted to our training set and come up with predictions on that test set. We can then use those predictions in order to see r2_score between the actual values and our predictions. Then just as we did before, we'll keep appending on the new scores into our scores list. Then we see here that our scores are exactly the same and that's because for vanilla linear regression, without any regularization, and that's going to be a term we'll introduce later on, but the idea there being that there's lasso and ridge, which we talked about, which will allow you to prevent overfitting. Those will need you to scale your data for regular linear regression. Scaling won't actually affect performance."]},{"cell_type":"code","execution_count":16,"id":"44c69e8e-00d2-4cde-9aa7-cbf4b6e4ff3e","metadata":{},"outputs":[],"source":["scores = []\n","\n","lr = LinearRegression()\n","s = StandardScaler()\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n","                                        X.iloc[test_index, :], \n","                                        y[train_index], \n","                                        y[test_index])\n","    \n","    X_train_s = s.fit_transform(X_train)\n","    \n","    lr.fit(X_train_s, y_train)\n","    \n","    X_test_s = s.transform(X_test)\n","    \n","    y_pred = lr.predict(X_test_s)\n","\n","    score = r2_score(y_test.values, y_pred)\n","    \n","    scores.append(score)"]},{"cell_type":"code","execution_count":17,"id":"681b7bfd-0dde-4899-abe7-7fb112860b07","metadata":{},"outputs":[{"data":{"text/plain":["[0.6719348798472715, 0.748502005921238, 0.6976807323597745]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["scores"]},{"cell_type":"markdown","id":"5f0e37bf-2e13-4149-82e5-a1eb798981e9","metadata":{},"source":["(same scores, because for vanilla linear regression with no regularization, scaling actually doesn't matter for performance)\n"]},{"cell_type":"markdown","id":"0bdbad18-f91b-4996-9c0d-48f461acd643","metadata":{},"source":["This is getting quite cumbersome! \n","\n","_Very_ luckily, SciKit Learn has some wonderful functions that handle a lot of this for us.\n","\n","As we add on more and more preprocessing steps, this can become pretty cumbersome to keep adding on and doing your fit transform, then your next fit transform if you have another preprocessing step, and then when you want to run it on the test having to transform each one of those. Luckily, Sklearn recognizes that this is common within the machine learning work for, and they have introduced the pipeline functionality. Pipelines are going to allow you to chain together multiple operators on your data as long as they each have a fit method. They all have to have a fit method, and then on top of that, every step leading up to the last step has to have a fit and transform so that the output of one can be the input of the next step. So you can chain together more than two steps. You can chain together 10 steps as long as each one of those steps has fit transform, and the last step has fit."]},{"cell_type":"markdown","id":"7e26cde3-8256-4f75-8b43-216bafc808c1","metadata":{},"source":["### `Pipeline` and `cross_val_predict`\n"]},{"cell_type":"markdown","id":"1b1d4b2d-4070-4c43-a52c-ac3adb2d6da1","metadata":{},"source":["`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method.\n","\n","We're going to reinitiate our StandardScaler and our linear regression, and then we're going to introduce our pipeline function."]},{"cell_type":"code","execution_count":18,"id":"8d4b5fae-afaf-4cff-a74f-b2b94da3fb98","metadata":{},"outputs":[],"source":["s = StandardScaler()\n","lr = LinearRegression()"]},{"cell_type":"markdown","id":"ca3cad34-87fb-4f7c-b8cd-e18e1604f45e","metadata":{},"source":["### Combine multiple processing steps into a `Pipeline`\n","\n","A pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.  \n","\n","Recall we imported pipeline earlier from sklearn.pipeline, and we're going to set this variable estimator equal to our pipeline. Our pipeline's going to have two steps. First, it will scale our data, then it will pass that through to a linear regression. This will allow us to bypass those steps that we saw above of fit transform as well as transform when we actually want to test."]},{"cell_type":"code","execution_count":19,"id":"659a5607-58c6-404f-8223-04be928d575e","metadata":{},"outputs":[],"source":["estimator = Pipeline([(\"scaler\", s),\n","                      (\"regression\", lr)])"]},{"cell_type":"markdown","metadata":{},"source":["To show you what that looks like, we're going to quickly introduce the code. We have estimator now created, that object is created. Similar to what we do with linear regression, we can actually just pass in our x_train and our y_train. We have now fit that pipeline objects specifically to the x_train and y_train. What we've done there is we first scaled our x_train data, and then we ran regression, so we fit the data according to the scaled version of the data."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n","                (&#x27;regression&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n","                (&#x27;regression&#x27;, LinearRegression())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div></div></div>"],"text/plain":["Pipeline(steps=[('scaler', StandardScaler()),\n","                ('regression', LinearRegression())])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["estimator.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Now that the estimator is fit to the data, we can even call estimator.predict, similar to what we do with linear regression and pass it in our x_test, and it will create an actual prediction."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["array([19.44230308, 22.8687781 , 20.72201438, 20.19685225, 16.02553328,\n","       13.19670707, 18.48717304, 16.41260539, 20.08040483, 19.16027923,\n","       23.03341265, 23.52809897, 24.64808538, 23.84554003, 22.37992895,\n","       18.54423788,  9.23453025, 24.86310535, 27.35965358, 29.95093715,\n","       21.71928179, 18.63649236, 19.67735609, 29.98064881, 20.78659698,\n","       17.29484103, 20.88767674, 23.59352298, 23.01287114, 24.82516061,\n","       28.1550891 , 26.59019417, 28.40170962, 28.61954803, 28.99730977,\n","       25.46130359, 20.83346878, 21.39384122, 21.12590183, 24.80991192,\n","       22.64896843, 20.63582629, 20.04498128, 20.28679691, 16.04272023,\n","       15.75494076, 12.78067624, 11.43558055, 21.98898014, 23.31730716,\n","       24.28692882, 21.28789025, 14.88646576, 31.00959189, 35.37282463,\n","       37.57270926, 23.58942636, 24.75302163, 30.63215847, 34.85770942,\n","       32.94090699, 30.0536187 , 39.86200512, 29.05010705, 35.53401305,\n","       41.89294309, 23.54417467, 23.61752172,  8.51966664, 24.3803473 ,\n","       31.93213161, 35.04907462, 33.74198666, 30.95975523, 38.16572411,\n","       34.04572219, 25.50003895, 26.9008276 , 26.81331202, 17.52450192,\n","       23.0561211 , 23.35940105, 27.41662679, 37.95283941, 35.39695531,\n","       37.15560829, 30.16078745, 31.7987809 , 22.17460829, 36.87065984,\n","       29.95618041, 35.45099005, 38.96516444, 19.04113986, 27.03723525,\n","       33.28276705, 19.71614458, 30.7356145 , 35.74753519, 33.31789906,\n","       29.82085216, 19.17675208, 26.73030054, 18.85119288, 25.10080811,\n","       25.2310572 , 23.35918589, 20.55011533, 23.44439899, 21.43873939,\n","       22.28755319, 21.55265486, 22.1886005 , 30.13309578, 27.73873544,\n","       27.83343144, 21.22476747, 20.29426743, 16.43151094, 24.1989263 ,\n","       23.86222031, 20.4568203 , 22.28180574, 36.99716281,  6.35326377,\n","       25.54462242, 15.91166325, 14.2172422 ,  8.43052398, 21.03393529,\n","       19.10628664, 19.46324302, 12.20521866, 18.2420435 , 18.49378935,\n","       21.90422664, 13.82169579, 19.42918743,  8.44262399,  7.195685  ,\n","       20.36534151, 19.00376186, 17.79256504, 17.87715422, 21.34551178,\n","        8.19204123,  3.32788461, 11.70535034, 11.96121376, 18.89797015,\n","       22.06307051, 14.60280556, 23.32232899, 14.26431606, 20.57202772,\n","       19.80582497, 27.50273944, 19.08219115, 21.53979588, 13.11731152,\n","       15.37086566, 20.96805073, 20.21749107, 19.62802415, 21.55434999,\n","       20.98843917, 23.32746873, 26.67017418])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["estimator.predict(X_test)"]},{"cell_type":"markdown","id":"e59b73e2-b8c0-4572-8c68-d2325258917f","metadata":{},"source":["### `cross_val_predict`\n","\n","[`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way.\n","\n","Now, rather than doing that for-loop, if we want to get the prediction for each one of our holdout sets in our K-folds, we can use this function called cross val predict. The way that works, first, let's reintroduce that. We have this K-fold objects, which will specify that we want three splits and that we want it to shuffle rather than doing it in some type of ordering."]},{"cell_type":"code","execution_count":22,"id":"4d5ef4f4-0a22-4544-abdf-3eb99993b585","metadata":{},"outputs":[{"data":{"text/plain":["KFold(n_splits=3, random_state=72018, shuffle=True)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["kf"]},{"cell_type":"markdown","metadata":{},"source":["We're going to pass in that K-folds object here into our cross val predict, which is why we reintroduced it. We're going to say, I want to form my estimator, and that's the pipeline that we created above. For my initial values of x, so no splits, the cross val predict will do the splitting for you. For my initial y, I want to pass in how many folds? So you can say I want the K-folds object that we specified here, which will ensure that you're not just passing in three splits, but also that it's a shuffle, it'll be a specific type of split. You could also just pass into here for your CV, the number 3, and that will create three splits, but they may not be shuffled. This allows you to specify exactly how you want it to split.\n","\n","Then when I run predictions, that will output predictions, giving that it's going to train on two-thirds and then predict for the other third. After it predicts all three of those thirds, it will have predicted for every single value in our dataset, but only using training from different subsets of that dataset. That's how the cross val predict works. We have our training set which is two-thirds, and a holdout set that is one-third. So we're predicting that one-third and then we're using a different two-thirds to predict another one-third."]},{"cell_type":"code","execution_count":23,"id":"d4ff0d96-fe79-4357-863d-6b2d526e6442","metadata":{},"outputs":[],"source":["predictions = cross_val_predict(estimator, X, y, cv=kf)"]},{"cell_type":"markdown","metadata":{},"source":["Now we have our predictions. I'm going to run the length of our predictions and you see that it's the same length as the length of our actual dataframe."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["506"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["len(predictions)"]},{"cell_type":"markdown","metadata":{},"source":["Then going to check the r2_score on our predictions compared to the original y, and we see that that's almost identical. I'm going to run that here. These are the scores that we had above in regards to calling out every single one, one at a time."]},{"cell_type":"code","execution_count":25,"id":"1d5afa8d-3348-4c53-86b0-35049dfcd6dc","metadata":{},"outputs":[{"data":{"text/plain":["0.7063531064161561"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["r2_score(y, predictions)"]},{"cell_type":"code","execution_count":26,"id":"a089f671-e738-4af9-837e-c3550069de1f","metadata":{},"outputs":[{"data":{"text/plain":["np.float64(0.7060392060427613)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(scores) # almost identical!"]},{"cell_type":"markdown","metadata":{},"source":["That's how the cross val predict will work. What's important to note is that the cross val predicts did not actually fit the model at any step along the way. It's going to give you all the outputs, but it essentially came up with three different models; one trained on the first two-thirds, one trained on the next two-thirds, and so on."]},{"cell_type":"markdown","id":"6a32bab2-4bfd-43f6-80ef-3cb895493a2d","metadata":{},"source":["Note that `cross_val_predict` doesn't use the same model for all steps; the predictions for each row are made when that row is in the validation set. We really have the collected results of 3 (i.e. `kf.num_splits`) different models. \n","\n","When we are done, `estimator` is still not fitted. If we want to predict on _new_ data, we still have to train our `estimator`. \n"]},{"cell_type":"markdown","id":"00f5dc44-32a7-4c1c-8b3c-6035eaaa017c","metadata":{},"source":["## Hyperparameter tuning\n","\n","As a quick recap, hyperparameters versus parameters, the hyperparameter's to be parts of our model that we as users will actually tune ourselves, versus parameters, which will be learned by the model using machine learning.\n","\n","Now, how do we come up with these hyperparameters in order to optimize the model performance? The way that we do this is going to be a hyperparameter tuning, and that's going to involve using cross-validation, so multiple train test splits, as we just did, in order to determine which hyperparameters are most likely to generalize well for an outside sample. So like we saw in lecture, we had that curve and regards to the complexity versus the error, we want to find that perfect part of the curve. So we'll test many different hyperparameters to see which one of those hyperparameters will lead to the right level of complexity to minimize our error on a holdout set. So generally speaking, tuning those hyperparameters will increase or decrease the level of complexity of your model."]},{"cell_type":"markdown","id":"23c1a676-2f42-41a4-9b4d-aefa2e098e15","metadata":{},"source":["### Definition\n","\n","**Hyperparameter tuning** involves using cross validation (or train-test split) to determine which hyperparameters are most likely to generate a model that _generalizes_ well outside of your sample.\n","\n","### Mechanics\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["So a quick introduction to a function that we're going to use here, the geomspace. The geomspace is just going to be, here we say 1 through 1,000 and the number of values you want is equal to 4. It's just that every single value in between will be a multiple of the prior value. So 10 is 10 times 1 and 100 is 10 times 10 and 1,000 is 10 times 100. You can think of this in the same way and np.geomspace. And I can say 1, 27, 4, and you see that each one's 3 times the prior value."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["array([ 1.,  3.,  9., 27.])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["np.geomspace(1, 27, 4)"]},{"cell_type":"markdown","metadata":{},"source":["We can generate an exponentially spaces range of values using the numpy [`geomspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html#numpy.geomspace) function.\n","\n","```python\n","np.geomspace(1, 1000, num=4)\n","```\n","\n","produces:\n","\n","```\n","array([    1.,    10.,   100.,  1000.])\n","```\n","\n","Use this function to generate a list of length 10 called `alphas` for hyperparameter tuning:\n","\n","So here we're going to create 10 value starting with 1 times e to the negative 9, 1 times 10 to the power of negative 9, which is going to be 0.00, that many zeros one, up until just one itself."]},{"cell_type":"code","execution_count":28,"id":"1e924295-d95f-4db0-b995-b4209a94bae3","metadata":{},"outputs":[{"data":{"text/plain":["array([1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,\n","       1.e-01, 1.e+00])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["alphas = np.geomspace(1e-9, 1e0, num=10)\n","alphas"]},{"cell_type":"markdown","metadata":{},"source":["So we see that, that this is to the negative 9, negative 8, negative 7, so on and so forth till we get to 0.01, 0.1, and then just 1.\n","\n","So we have not yet introduced lasso, and all I mentioned before and all you need to know here is that changing this alpha value within lasso. So we're going to use this lasso model and we're going to initiate the same way we do with linear regression. And we're going to pass in this argument to alpha for each one of the alphas that we defined before. All you need to know is that the higher the alpha is, the less complex your model is. The lower the alpha is, the more complex it is, and the closer it is to regular linear regression. And this will become even clearer as we go through the outputs of the actual coefficients for each one of our features that we have in our data set."]},{"cell_type":"markdown","id":"d3d5a4eb-e9e3-4a12-95d5-f9dcfe20ce19","metadata":{},"source":["The code below tunes the `alpha` hyperparameter for Lasso regression.\n","\n","So we're starting off with scores is equal to a blank list, and the coefficients is equal to a blank list. And what we're going to want to do is actually see which one of our different alpha values is going to lead to the highest score for our holdout set. So that's going to allow us to look through each one of these hyperparameters, each one of them being an alpha, and see how much do we need to limit the complexity of our model. So we, for every alpha, initiate our lasso model.\n","\n","We're then going to run our estimator, create our estimator using pipeline.\n","\n","Now, we'll get into this later as well, but it's very important whenever you're doing lasso or ridge regression, and we'll show you ridge regression in a second, that you scale your data first. Always have to scale your data, otherwise the model will not work optimally.\n","\n","So we scale it, then we run lasso regression, so similar to the pipeline that we saw above.\n","\n","And then we just get our predictions using cross val predicts. We pass in our estimator, our x, our y, and how we want to do cross validation. And then we get our r2 score and append that to our scores. So we're going to do that for each one of our alphas."]},{"cell_type":"code","execution_count":29,"id":"76f4064d-b0ac-4a6e-a07b-7d94ef7c5264","metadata":{},"outputs":[],"source":["scores = []\n","coefs = []\n","for alpha in alphas:\n","    las = Lasso(alpha=alpha, max_iter=100000)\n","    \n","    estimator = Pipeline([\n","        (\"scaler\", s),\n","        (\"lasso_regression\", las)])\n","\n","    predictions = cross_val_predict(estimator, X, y, cv = kf)\n","    \n","    score = r2_score(y, predictions)\n","    \n","    scores.append(score)"]},{"cell_type":"markdown","metadata":{},"source":["And then we can look at for each one of our alphas what our score was. So this is the most complex of the models, and this is the least complex. And we see that it performed, it has a pretty flat curve in regards to how much lowering complexity is actually helping in regards to our holdout set."]},{"cell_type":"code","execution_count":30,"id":"8e86b541-0a48-43e1-8036-f0bda3e3d19a","metadata":{},"outputs":[{"data":{"text/plain":["[(np.float64(1e-09), 0.7063531064981925),\n"," (np.float64(1e-08), 0.7063531072356071),\n"," (np.float64(1e-07), 0.7063531145602442),\n"," (np.float64(1e-06), 0.7063531882052063),\n"," (np.float64(1e-05), 0.7063539165191507),\n"," (np.float64(0.0001), 0.706361268093463),\n"," (np.float64(0.001), 0.706433467041546),\n"," (np.float64(0.01), 0.7070865958083233),\n"," (np.float64(0.1), 0.705838151167185),\n"," (np.float64(1.0), 0.6512724532884888)]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["list(zip(alphas,scores))"]},{"cell_type":"markdown","metadata":{},"source":["So, Just to make clear how we are actually making it more or less complex of a model. If we look at the coefficients for very little alphas, for those that will reduce complexity much less, you see that our coefficients are all different numerical values. Whereas if I do alpha equals 1, which is much higher alpha value, so much less complexity, we end up removing many of our coefficients, right? These coefficients are each going to relate to a feature, and we have removed those features, essentially, from our prediction values."]},{"cell_type":"code","execution_count":31,"id":"41845885-4b0e-45ab-b7c0-77be9b59746d","metadata":{},"outputs":[{"data":{"text/plain":["array([-1.07170372e-01,  4.63952623e-02,  2.08588308e-02,  2.68854318e+00,\n","       -1.77954207e+01,  3.80475296e+00,  7.50802707e-04, -1.47575348e+00,\n","        3.05654279e-01, -1.23293755e-02, -9.53459908e-01,  9.39253013e-03,\n","       -5.25467196e-01])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["Lasso(alpha=1e-6).fit(X, y).coef_"]},{"cell_type":"code","execution_count":32,"id":"14d13291-e83e-498e-a7ee-16679b5cdb3b","metadata":{},"outputs":[{"data":{"text/plain":["array([-0.06342255,  0.04916867, -0.        ,  0.        , -0.        ,\n","        0.94678567,  0.02092737, -0.66900864,  0.26417501, -0.01520915,\n","       -0.72319901,  0.00829117, -0.76143296])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["Lasso(alpha=1.0).fit(X, y).coef_"]},{"cell_type":"markdown","metadata":{},"source":["And I'm going to plot this and we're going to be able to see the trade-off between higher complexity and our error rate. And we see that it's fairly level throughout.\n","\n","So we probably do not need as much regularization as we're just working with plain lasso with just the standard scalar. We're going to see in a second, as we end up with many more features using something like polynomial features, how we'll need to actually reduce the complexity, and probably move many of these actual interaction terms or squared values."]},{"cell_type":"code","execution_count":33,"id":"d0e2e93a-cff9-4510-b0cd-1b06692b9852","metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.figure(figsize=(10,6))\n","plt.semilogx(alphas, scores, '-o')\n","plt.xlabel('$\\\\alpha$')\n","plt.ylabel('$R^2$');"]},{"cell_type":"markdown","id":"f4e7136f-7ecf-4ba8-8921-abe389a6f772","metadata":{},"source":["### Exercise\n","\n","Add `PolynomialFeatures` to this `Pipeline`, and re-run the cross validation with the `PolynomialFeatures` added.\n","\n","**Hint #1:** pipelines process input from first to last. Think about the order that it would make sense to add Polynomial Features to the data in sequence and add them in the appropriate place in the pipeline.\n","\n","**Hint #2:** you should see a significant increase in cross validation accuracy from doing this\n","\n","So if you think about it, now there's a little bit of an argument here. But on one side, if you standardize first, then you will end up with negative and positive values for values that may have all been positive, therefore changing all of your interaction terms. Also, you're going to be bringing them down to a much smaller scale. So rather than the first value times the second value both being above 1 and therefore increasing that vale even mor. If you scale it down to values that are maybe 0.5 and 2, we're actually reducing that value.\n","\n","So that's on one end of the argument why we would want to do polynomial features first, and then scale it down.\n","\n","Also on top of that, scaling it second will also ensure that each one of them are on the same scale at the end.\n","\n","On the other hand, you may want to know the interaction between the scale data so that you'd be able to see, just in regards to how much one varies from the mean, what will be the interaction versus the squared of those values.\n","\n","But we're going to stick here with just the polynomial features first, which is generally regarded as best practice.\n","\n","We're going to initiate our polynomial features object. We're going to start off with a blank list of scores, as we did before. We're going to have our list of alphas. And then for each one of those alphas, we're going to do the same as we did in regards to initiating our lasso object. Now, I do want to say that we have this max iteration, which we didn't discuss before. The idea being that when you're working with lasso regression, it's actually going to work with radiant descent, which we haven't introduced yet. The idea is for it to get to the optimal value, it's essentially running in the back end, what you can think of as for loop, as it gets closer and closer to the correct value. And it's going to do that in small little steps, and if you don't have enough iterations, and the default here is not enough iterations, then it won't get to that optimal value. So that's why we have to set the max iterations.\n","\n","We're then creating our estimator object, which starts with the polynomial features, then scales it, and then pass that through to our lasso regression.\n","\n","We're then going to come up with predictions by just running cross val predicts, passing in our estimator, our x and our y. And then we're going to get our new score and we're going to append each one of these scores. Now, running this will take about 30 seconds. Also note that you will probably get a warning giving the max iterations that we have here, that it hasn't converged. But we tested this on the back end to ensure that the numbers are fairly close to the actual optimal value. So I'm going to take a break here, and we'll come back as soon as this is done running."]},{"cell_type":"code","execution_count":34,"id":"ccc6f99c-a5f6-4bec-8696-faba60ededac","metadata":{},"outputs":[{"data":{"text/plain":["[0.8465056899967298,\n"," 0.8552553819994727,\n"," 0.8124536477296069,\n"," 0.7090474482089187,\n"," -0.000616160136140298]"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["pf = PolynomialFeatures(degree=2)\n","scores = []\n","\n","alphas = np.geomspace(0.001, 10, 5)\n","for alpha in alphas:\n","    las = Lasso(alpha=alpha, max_iter=100000)\n","    estimator = Pipeline([\n","        (\"make_higher_degree\", pf),\n","        (\"scaler\", s),\n","        (\"lasso_regression\", las)])\n","    predictions = cross_val_predict(estimator, X, y, cv = kf)\n","    score = r2_score(y, predictions)\n","    \n","    scores.append(score)\n","scores"]},{"cell_type":"markdown","metadata":{},"source":["Okay, now our functions have ran and we can look at for each one of our alpha values what are different scores were. And we see, starting off with very little alphas, so very high complexities that we may not be generalizing well. We see with a little bit more of a little bit higher of an alpha, then we are generalizing better. And then as that alpha gets even higher, we may have reached our peak."]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["alphas"]},{"cell_type":"markdown","id":"b2f75c19-2e6a-4644-aa61-4318f6545e43","metadata":{},"source":["If you store the results in a list called `scores`, the following will work:\n"]},{"cell_type":"code","execution_count":36,"id":"a9c7ca3b-59fa-4362-b27e-a272ba126ec4","metadata":{},"outputs":[{"data":{"text/plain":["[<matplotlib.lines.Line2D at 0x28abc260cd0>]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.semilogx(alphas, scores)"]},{"cell_type":"markdown","metadata":{},"source":["So to look at this graph we would say that at this 0.01 we probably got our optimal value in regards to the hyperparameter that will generalize well to new new data coming in."]},{"cell_type":"markdown","metadata":{},"source":["So with that, we can then use that hyperparameter to train our actual model. So we're going to set our best estimator equal to this pipeline where your polynomial features degree equals 2. Then scaling, and then using the alpha that we found was the best value with 0.01, and we can fit that. We see what the score was here for the best estimator, which will be built in, and it will be by default the r2 score. And when we are looking at that, we have to keep in mind also that we are training and scoring on the same data set. So generally speaking, as usual, we probably want to have a holdout set. But this is just to show us how much of our variation we were actually able to cover given the model that we used."]},{"cell_type":"code","execution_count":37,"id":"035ad8e9-1ba3-4e41-a9e9-b54d4103c610","metadata":{},"outputs":[{"data":{"text/plain":["0.9032551408154241"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# Once we have found the hyperparameter (alpha~1e-2=0.01)\n","# make the model and train it on ALL the data\n","# Then release it into the wild .....\n","best_estimator = Pipeline([\n","                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n","                    (\"scaler\", s),\n","                    (\"lasso_regression\", Lasso(alpha=0.01, max_iter=10000))])\n","\n","best_estimator.fit(X, y)\n","best_estimator.score(X, y)"]},{"cell_type":"markdown","metadata":{},"source":["And then when we look at the coefficients, because we used the lasso regression and we added this alpha term, we actually removed many of these features, as you see a lot of them essentially zeroed out."]},{"cell_type":"code","execution_count":38,"id":"0c20acfd-4f0e-4c5d-b20d-5eef0b60d777","metadata":{},"outputs":[{"data":{"text/plain":["array([ 0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n","        4.61127587e+00,  0.00000000e+00,  6.24240557e-01,  0.00000000e+00,\n","       -7.22878514e+00,  9.74968793e+00,  0.00000000e+00,  0.00000000e+00,\n","        6.34548325e-01,  0.00000000e+00,  9.95741679e-01, -5.51374291e-03,\n","       -0.00000000e+00,  2.44370838e+00, -2.40444044e+00, -0.00000000e+00,\n","       -0.00000000e+00, -2.23044443e-01, -0.00000000e+00, -0.00000000e+00,\n","       -0.00000000e+00, -2.88128934e-01,  5.17203914e-01,  3.07715719e-01,\n","       -3.39853933e-01, -9.86590910e-02, -0.00000000e+00,  2.72847487e-01,\n","       -0.00000000e+00, -6.39829763e-01, -3.16474346e-01,  1.44555554e+00,\n","        0.00000000e+00, -0.00000000e+00, -8.84630187e-01,  1.32081558e+00,\n","        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  2.63148300e+00,\n","       -2.62941147e-01,  0.00000000e+00,  9.08410143e-01, -8.17144528e-01,\n","        0.00000000e+00, -4.31628234e+00,  5.53254405e-01, -2.91912598e+00,\n","       -4.68486241e+00,  5.80360142e-01,  6.93705854e-01, -1.43087388e+00,\n","       -0.00000000e+00,  0.00000000e+00,  1.80219873e+00, -7.93324201e-01,\n","        0.00000000e+00, -2.99366807e-01, -0.00000000e+00, -0.00000000e+00,\n","       -9.26619562e-01, -0.00000000e+00, -1.52207140e+00,  0.00000000e+00,\n","        1.59132103e-01,  8.59406852e+00, -1.79580442e+00,  0.00000000e+00,\n","       -4.43064063e+00, -6.67704537e+00, -2.78398024e+00,  4.36511083e-01,\n","       -2.07418701e+00, -0.00000000e+00,  4.42278783e-01,  1.97121461e+00,\n","        0.00000000e+00, -0.00000000e+00, -5.63885266e-01, -1.90366836e+00,\n","        3.79255941e+00, -0.00000000e+00, -3.58998427e-01,  3.99107900e-01,\n","       -0.00000000e+00,  8.70546159e-01, -0.00000000e+00,  0.00000000e+00,\n","        0.00000000e+00,  0.00000000e+00, -4.54568143e+00,  0.00000000e+00,\n","        5.12959518e+00,  0.00000000e+00, -6.41075173e-01,  3.19584847e-01,\n","        1.08275771e+00,  0.00000000e+00, -4.07836880e-01, -1.15427697e+00,\n","        5.63522883e+00])"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["best_estimator.named_steps[\"lasso_regression\"].coef_"]},{"cell_type":"markdown","id":"4d5124bb-24a1-4c4d-bb42-85961bbe4662","metadata":{},"source":["### Exercise\n","\n","Do the same, but with `Ridge` regression \n","\n","Which model, `Ridge` or `Lasso`, performs best with its optimal hyperparameters on the Boston dataset?\n"]},{"cell_type":"markdown","metadata":{},"source":["Here we're going to briefly introduce ridge regression. Now again, we haven't gone through the math of ridge and lasso regression. All we need to know is that these are two different forms of reducing the complexity of the original linear regression model. So that's all we'll know for now, we'll get deeper into the math in lecture. Now as before, before passing through to our new model here being the ridge regression, we're going to create our PolynomialFeatures object, we're going to add certain Alphas that we're going to search through, and we're going to initiate just blank scores that we will append as we get the scores for each one of our ridge regressions given the different hyperparameters that we're passing through. Again, our goal is to find the optimal hyperparameters so that it generalizes well to new data. So we don't want it to be too complex, but we don't want it to be a minimal amount of complexity. We want that just write region and we do that by doing cross-validation and saying for each one of these Alphas, either reducing complexity a turn or just a bit, where is the right balance in regards to complexity to minimize the error on our holdout sets. We're doing for Alpha in our Alphas, we run our ridge regression and for each one we pass in this hyperparameter of our Alpha. We also are passing in this max iteration to ensure that it will converge as we discussed. We will initiate our estimator object again using a pipeline of polynomial_features, the standard scalar, and then ultimately our ridge_regression, which we defined above and then we'll get our predictions using the cross _val_predict on each one of our holdout sets and then we can see the r2_scores for each and append that onto our scores. Then we'll plot in order to see how the Alphas versus the scores, how that's going to increase or decrease our r2_score as we increase or decrease our complexity using these Alphas."]},{"cell_type":"code","execution_count":39,"id":"9acfc597-8bef-4e7d-8129-c2ffff7298f4","metadata":{},"outputs":[{"data":{"text/plain":["[<matplotlib.lines.Line2D at 0x28a9426a490>]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["pf = PolynomialFeatures(degree=2)\n","scores = []\n","\n","alphas = np.geomspace(0.1, 2, 20)\n","for alpha in alphas:\n","    ridge = Ridge(alpha=alpha, max_iter=100000)\n","\n","    estimator = Pipeline([\n","        (\"polynomial_features\", pf),\n","        (\"scaler\", s),\n","        (\"ridge_regression\", ridge)])\n","\n","    predictions = cross_val_predict(estimator, X, y, cv = kf)\n","    score = r2_score(y, predictions)\n","    scores.append(score)\n","plt.plot(alphas, scores)"]},{"cell_type":"markdown","metadata":{},"source":["Recall that a lower Alpha means more complexity. So we see all the way here to the left with much lower Alpha, we are not getting as high of an r2_score but as we increase the Alpha and reduce the complexity, we have this optimal points around 0.75. So that would be the optimal hyperparameter. Anything to the right of that is probably not complex enough. So we have that just write point at around 0.75. Our conclusion that we can get from this, from both of these curves having this upward trajectory and then this downward slope, is that using this Alpha value in order to reduce complexity, again, no Alpha value at all would be essentially the same as linear regression. So we can see that as we reduce complexity slightly, we're actually able to improve how well we are able to generalize our model. Reducing complexity even on a simple model such as linear regression is doing well in regards to optimizing how well we will perform on our holdout sets."]},{"cell_type":"markdown","id":"5ac69437-cea5-43f0-a0a6-582f0e53a312","metadata":{},"source":["**Conclusion:** Both Lasso and Ridge with proper hyperparameter tuning give better results than plain ol' Linear Regression!\n"]},{"cell_type":"markdown","id":"33127b0c-53b4-4c21-91a2-72cbdaa8a388","metadata":{},"source":["### Exercise:\n"]},{"cell_type":"markdown","id":"2eda7ba6-9ad7-4616-b964-760ea3174c7d","metadata":{},"source":["Now, for whatever your best overall hyperparameter was: \n","\n","* Standardize the data\n","* Fit and predict on the entire dataset\n","* See what the largest coefficients were\n","    * Hint: use \n","    ```python\n","    dict(zip(model.coef_, pf.get_feature_names_out()))\n","    ```\n","    for your model `model` to get the feature names from `PolynomialFeatures`.\n","    \n","    Then, use\n","    ```python\n","    dict(zip(list(range(len(X.columns.values))), X.columns.values))\n","    ```\n","    \n","    to see which features in the `PolynomialFeatures` DataFrame correspond to which columns in the original DataFrame.\n"]},{"cell_type":"markdown","metadata":{},"source":["Now I want to go over how we can look at some feature importances. So looking at interpretability. What is important? Whenever we want to look at interpretability for something like linear regression, we need to ensure that all of our features are on the same scale. If you think about one of them being values between zero and five and the other one being values between 10,000 and 100,000, for trying to predict how much a unit change of 0-5 will affect median value, a one unit change will probably have a larger effect, whereas a one unit change in values between 10,000 and 100,000 will have a very little effects on the overall median value of our households. So what we want to do is bring those all down to the same scale so that our coefficients are actually measuring for one unit change in standard deviation. So as it varies in accordance to the variation built into that feature, how much will that affect our median household value. So now they're all on the same scale. The larger that coefficient is, the more important that feature can be seen in regards to predicting the median household value.\n","\n","The way that we're going to do that is again, first standardize the data. We're then going to fit and predict on the entire dataset. So we're going to get our training set, because we don't care as much about prediction here, but rather interpretability. We want to bring in all of our training data and then want to see which one of our values have the largest coefficients.\n","\n","Now in order to do this, we're going to have to run this code that we see here, and I'll explain that clearly as we go through the code in just a couple cells. So we have our best hyperparameter with lasso, with Alpha equal to 0.01. We create our pipeline where we create our polynomial features. So we're going to have a ton of new features as we square each one of our features, as well as creating all of the interaction terms between each one of our features. Then we're going to scale it, so they're all on the same scale, and then we're going to fit the X and y. Again, we don't care as much about prediction, we've already gotten the 0.01, is going to be our optimal value in regards to how well we'll perform on a holdout set. Now, we want to see what the actual interpretability of that output will be. So we're fitting on the entire training set to get the coefficients fit across as much data as possible. We'll look at the score here that won't be as important, and then we're going to look at each one of the feature importances using the code that we see above."]},{"cell_type":"code","execution_count":40,"id":"cff7875d-2a9e-48a1-a646-1e849b23c626","metadata":{},"outputs":[{"data":{"text/plain":["0.9032551408154241"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# Once we have found the hyperparameter (alpha~1e-2=0.01)\n","# make the model and train it on ALL the data\n","# Then release it into the wild .....\n","best_estimator = Pipeline([\n","                    (\"make_higher_degree\", PolynomialFeatures(degree=2, include_bias=False)),\n","                    (\"scaler\", s),\n","                    (\"lasso_regression\", Lasso(alpha=0.01, max_iter=10000))])\n","\n","best_estimator.fit(X, y)\n","best_estimator.score(X, y)"]},{"cell_type":"markdown","metadata":{},"source":["So step-by-step, when we run best_estimator here, let's pull this all out. So I promised you that later on we would take from the pipeline the actual names and we will actually make use of each of these names that we pass through, rather than just passing through perhaps polynomial features or our StandardScaler or whatever our object is that we are passing through the pipeline. If we want to access a portion of the pipeline, then we have to take our best_estimator, which is equal to this pipeline that we have fit before and call out.named_steps. That.named_steps will allow us to access a dictionary where we can look at different attributes of each of these subsets of our pipeline. So each one of these subsets could be either the polynomial features or the StandardScaler or if we want the coefficients, we get both from the lasso here. So we do best_estimator.named_steps. We're pulling outs the polynomial features here and we're calling this function called get_feature_names, and that's going to allow us to get the X1, X2, X3, and those values squared so that we'll be able to look back and see which one of our values actually ended up having the highest feature importance. So you see here I'm pulling out all the names where we have CRIM and then we have CRIM squared, then CRIM time zone, and so on and so forth. So this allows us to see each one of our new features that we created using the polynomial features."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n","       'TAX', 'PTRATIO', 'B', 'LSTAT', 'CRIM^2', 'CRIM ZN', 'CRIM INDUS',\n","       'CRIM CHAS', 'CRIM NOX', 'CRIM RM', 'CRIM AGE', 'CRIM DIS',\n","       'CRIM RAD', 'CRIM TAX', 'CRIM PTRATIO', 'CRIM B', 'CRIM LSTAT',\n","       'ZN^2', 'ZN INDUS', 'ZN CHAS', 'ZN NOX', 'ZN RM', 'ZN AGE',\n","       'ZN DIS', 'ZN RAD', 'ZN TAX', 'ZN PTRATIO', 'ZN B', 'ZN LSTAT',\n","       'INDUS^2', 'INDUS CHAS', 'INDUS NOX', 'INDUS RM', 'INDUS AGE',\n","       'INDUS DIS', 'INDUS RAD', 'INDUS TAX', 'INDUS PTRATIO', 'INDUS B',\n","       'INDUS LSTAT', 'CHAS^2', 'CHAS NOX', 'CHAS RM', 'CHAS AGE',\n","       'CHAS DIS', 'CHAS RAD', 'CHAS TAX', 'CHAS PTRATIO', 'CHAS B',\n","       'CHAS LSTAT', 'NOX^2', 'NOX RM', 'NOX AGE', 'NOX DIS', 'NOX RAD',\n","       'NOX TAX', 'NOX PTRATIO', 'NOX B', 'NOX LSTAT', 'RM^2', 'RM AGE',\n","       'RM DIS', 'RM RAD', 'RM TAX', 'RM PTRATIO', 'RM B', 'RM LSTAT',\n","       'AGE^2', 'AGE DIS', 'AGE RAD', 'AGE TAX', 'AGE PTRATIO', 'AGE B',\n","       'AGE LSTAT', 'DIS^2', 'DIS RAD', 'DIS TAX', 'DIS PTRATIO', 'DIS B',\n","       'DIS LSTAT', 'RAD^2', 'RAD TAX', 'RAD PTRATIO', 'RAD B',\n","       'RAD LSTAT', 'TAX^2', 'TAX PTRATIO', 'TAX B', 'TAX LSTAT',\n","       'PTRATIO^2', 'PTRATIO B', 'PTRATIO LSTAT', 'B^2', 'B LSTAT',\n","       'LSTAT^2'], dtype=object)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["best_estimator.named_steps[\"make_higher_degree\"].get_feature_names_out()"]},{"cell_type":"markdown","metadata":{},"source":["Then we also want to pull out the coefficients that we learned with our model. So I'm going to copy this. Again, we use that.named_steps and we say that we want to pull from the lasso_regression. The lasso_regression is going to be the portion of our pipeline that contains the attribute of coefficients, and we see a coefficient for each one of those values that we just saw laid out with our polynomial features."]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":["array([-0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  4.61127587e+00,\n","        0.00000000e+00,  6.24240557e-01,  0.00000000e+00, -7.22878514e+00,\n","        9.74968793e+00,  0.00000000e+00,  0.00000000e+00,  6.34548325e-01,\n","        0.00000000e+00,  9.95741679e-01, -5.51374291e-03, -0.00000000e+00,\n","        2.44370838e+00, -2.40444044e+00, -0.00000000e+00, -0.00000000e+00,\n","       -2.23044443e-01, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n","       -2.88128934e-01,  5.17203914e-01,  3.07715719e-01, -3.39853933e-01,\n","       -9.86590910e-02, -0.00000000e+00,  2.72847487e-01, -0.00000000e+00,\n","       -6.39829763e-01, -3.16474346e-01,  1.44555554e+00,  0.00000000e+00,\n","       -0.00000000e+00, -8.84630187e-01,  1.32081558e+00,  0.00000000e+00,\n","        0.00000000e+00, -0.00000000e+00,  2.63148300e+00, -2.62941147e-01,\n","        0.00000000e+00,  9.08410143e-01, -8.17144528e-01,  0.00000000e+00,\n","       -4.31628234e+00,  5.53254405e-01, -2.91912598e+00, -4.68486241e+00,\n","        5.80360142e-01,  6.93705854e-01, -1.43087388e+00, -0.00000000e+00,\n","        0.00000000e+00,  1.80219873e+00, -7.93324201e-01,  0.00000000e+00,\n","       -2.99366807e-01, -0.00000000e+00, -0.00000000e+00, -9.26619562e-01,\n","       -0.00000000e+00, -1.52207140e+00,  0.00000000e+00,  1.59132103e-01,\n","        8.59406852e+00, -1.79580442e+00,  0.00000000e+00, -4.43064063e+00,\n","       -6.67704537e+00, -2.78398024e+00,  4.36511083e-01, -2.07418701e+00,\n","       -0.00000000e+00,  4.42278783e-01,  1.97121461e+00,  0.00000000e+00,\n","       -0.00000000e+00, -5.63885266e-01, -1.90366836e+00,  3.79255941e+00,\n","       -0.00000000e+00, -3.58998427e-01,  3.99107900e-01, -0.00000000e+00,\n","        8.70546159e-01, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n","        0.00000000e+00, -4.54568143e+00,  0.00000000e+00,  5.12959518e+00,\n","        0.00000000e+00, -6.41075173e-01,  3.19584847e-01,  1.08275771e+00,\n","        0.00000000e+00, -4.07836880e-01, -1.15427697e+00,  5.63522883e+00])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["best_estimator.named_steps[\"lasso_regression\"].coef_"]},{"cell_type":"markdown","metadata":{},"source":["We're going to zip those two together so that for each one of those features, it is perfectly aligned with each one of these coefficients. So that's going to be this zip function, and then we're going to pass it into a DataFrame and call that DataFrame df_importances. So what does our df_importances look like now? It's now the first column is going to be each one of our feature names, and our second column is going to be the magnitude of that coefficient."]},{"cell_type":"code","execution_count":43,"id":"04d807c1-4da5-4540-b539-446b8598a4c9","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CRIM</td>\n","      <td>-0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ZN</td>\n","      <td>-0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>INDUS</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CHAS</td>\n","      <td>4.611276</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NOX</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>PTRATIO B</td>\n","      <td>1.082758</td>\n","    </tr>\n","    <tr>\n","      <th>100</th>\n","      <td>PTRATIO LSTAT</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>101</th>\n","      <td>B^2</td>\n","      <td>-0.407837</td>\n","    </tr>\n","    <tr>\n","      <th>102</th>\n","      <td>B LSTAT</td>\n","      <td>-1.154277</td>\n","    </tr>\n","    <tr>\n","      <th>103</th>\n","      <td>LSTAT^2</td>\n","      <td>5.635229</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>104 rows × 2 columns</p>\n","</div>"],"text/plain":["                 0         1\n","0             CRIM -0.000000\n","1               ZN -0.000000\n","2            INDUS  0.000000\n","3             CHAS  4.611276\n","4              NOX  0.000000\n","..             ...       ...\n","99       PTRATIO B  1.082758\n","100  PTRATIO LSTAT  0.000000\n","101            B^2 -0.407837\n","102        B LSTAT -1.154277\n","103        LSTAT^2  5.635229\n","\n","[104 rows x 2 columns]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["df_importances = pd.DataFrame(zip(best_estimator.named_steps[\"make_higher_degree\"].get_feature_names_out(),\n","                 best_estimator.named_steps[\"lasso_regression\"].coef_,\n","))\n","\n","df_importances"]},{"cell_type":"code","execution_count":44,"id":"fa9f4f84-ca0c-49e7-a611-991f099067de","metadata":{},"outputs":[],"source":["col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))"]},{"cell_type":"code","execution_count":45,"id":"0ca93439-3c92-45e8-9e65-5fa40506b3d7","metadata":{},"outputs":[{"data":{"text/plain":["{0: 'CRIM',\n"," 1: 'ZN',\n"," 2: 'INDUS',\n"," 3: 'CHAS',\n"," 4: 'NOX',\n"," 5: 'RM',\n"," 6: 'AGE',\n"," 7: 'DIS',\n"," 8: 'RAD',\n"," 9: 'TAX',\n"," 10: 'PTRATIO',\n"," 11: 'B',\n"," 12: 'LSTAT'}"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["col_names_dict"]},{"cell_type":"markdown","metadata":{},"source":["We're then going to sort our values so we can see the highest negative and the highest positive values in regards to which variation affected our predicted outcome the most. Then we see the interaction between number of rooms and the tax rate has actually very negative effects, whereas we can actually pull out the boston_description that we defined earlier to see each one of these different column names if we are more curious.\n","\n","So we had that RAD times PTRATIO, which is going to be the index of accessibility to radial highways to PTRATIO, the pupil-teacher ratio by town. It's a little bit confusing to think that that would have such a large effects. We may want to test removing this as we have so many more features at this point. We're now working with 104 features or we can say maybe there is that effect and dive deeper in. Then the other thing that we can look at here is that room squared. So as the number of rooms increases, that's going to have more and more value in regards to how much we would predict the outcome variable that we're looking at."]},{"cell_type":"code","execution_count":46,"id":"5a3d28b7-0c15-4f15-af38-b243b3ade6b6","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>DIS</td>\n","      <td>-7.228785</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>RM TAX</td>\n","      <td>-6.677045</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>CHAS RM</td>\n","      <td>-4.684862</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>RAD LSTAT</td>\n","      <td>-4.545681</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>RM RAD</td>\n","      <td>-4.430641</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CHAS</td>\n","      <td>4.611276</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>TAX PTRATIO</td>\n","      <td>5.129595</td>\n","    </tr>\n","    <tr>\n","      <th>103</th>\n","      <td>LSTAT^2</td>\n","      <td>5.635229</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>RM^2</td>\n","      <td>8.594069</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>RAD</td>\n","      <td>9.749688</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>104 rows × 2 columns</p>\n","</div>"],"text/plain":["               0         1\n","7            DIS -7.228785\n","72        RM TAX -6.677045\n","51       CHAS RM -4.684862\n","93     RAD LSTAT -4.545681\n","71        RM RAD -4.430641\n","..           ...       ...\n","3           CHAS  4.611276\n","95   TAX PTRATIO  5.129595\n","103      LSTAT^2  5.635229\n","68          RM^2  8.594069\n","8            RAD  9.749688\n","\n","[104 rows x 2 columns]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["df_importances.sort_values(by=1)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Boston House Prices dataset\n","===========================\n","\n","Notes\n","------\n","Data Set Characteristics:  \n","\n","    :Number of Instances: 506 \n","\n","    :Number of Attributes: 13 numeric/categorical predictive\n","    \n","    :Median Value (attribute 14) is usually the target\n","\n","    :Attribute Information (in order):\n","        - CRIM     per capita crime rate by town\n","        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n","        - INDUS    proportion of non-retail business acres per town\n","        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","        - NOX      nitric oxides concentration (parts per 10 million)\n","        - RM       average number of rooms per dwelling\n","        - AGE      proportion of owner-occupied units built prior to 1940\n","        - DIS      weighted distances to five Boston employment centres\n","        - RAD      index of accessibility to radial highways\n","        - TAX      full-value property-tax rate per $10,000\n","        - PTRATIO  pupil-teacher ratio by town\n","        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n","        - LSTAT    % lower status of the population\n","        - MEDV     Median value of owner-occupied homes in $1000's\n","\n","    :Missing Attribute Values: None\n","\n","    :Creator: Harrison, D. and Rubinfeld, D.L.\n","\n","This is a copy of UCI ML housing dataset.\n","http://archive.ics.uci.edu/ml/datasets/Housing\n","\n","\n","This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n","\n","The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n","prices and the demand for clean air', J. Environ. Economics & Management,\n","vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n","...', Wiley, 1980.   N.B. Various transformations are used in the table on\n","pages 244-261 of the latter.\n","\n","The Boston house-price data has been used in many machine learning papers that address regression\n","problems.   \n","     \n","**References**\n","\n","   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n","   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n","   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n","\n"]}],"source":["print(boston_description)"]},{"cell_type":"markdown","id":"c50ef201-15b4-4f9b-9c61-9b81cf02f699","metadata":{},"source":["## Grid Search CV\n","\n","In the last section of this notebook, we're going to show you how we can use Grid Search CV to take everything that we just learnt with all the four loops, how we went with four loops for the Kfolds, and how we pass that through to cross val predict to make that more succinct. Now we are using the four loop through each one of our hyper-parameters, and then passing through cross val predicts, how we can do all of that in just one step using Grid Search CV."]},{"cell_type":"markdown","id":"65aa65b5-e923-4139-86fe-867b8c75725d","metadata":{},"source":["To do cross-validation, we used two techniques:\n","- use `KFolds` and manually create a loop to do cross-validation\n","- use `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.\n","\n","To do hyper-parameter tuning, we see a general pattern:\n","- use `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.\n","\n","Perhaps not surprisingly, there is a function that does this for us -- `GridSearchCV`\n","\n","So we're going to import from sklearn.model selection our Grid Search CV. We are going to initiate our estimator objects using pipeline. So our pipeline is going to be the polynomial features, then the scalar, then ridge regression here. Then we are going to say what our parameters are going to be. Now, what are going to be the hyper-parameters that we want to loop over in order to figure out which one of the hyper-parameters, again, those hyper-parameters being the ones that we choose rather than what the model learns that will optimize the results for a holdout set. So we are actually going to look at both polynomial features as well as the different Alpha values that we saw. So higher polynomial features will mean more complexity, and then a higher Alpha value will mean less complexity, and we will look at looping through every single one of those values. So if polynomial_features__degree is 3 and ridge_regression__alpha is 30, we are actually going to be looking at 90 different values, and we're going to run Grid Search CV. We have our estimator that we have defined here. So that's going to be our model, and then our parameters. Just so you know, just all practice when you're using Grid Search CV is going to be a dictionary where you're going to have as each one of the values, the different hyper-parameters that you can loop through. So the different hyper-parameters that we can loop through here are the different degrees. First we have to use the name that we defined up here, which is polynomial features, and then two underscores, and then we can pass in any of the hyper-parameters that are available for polynomial features. So that's the way that we decide how to name this. Then the same thing goes for ridge where we have to just use hyper-parameters that are available within the initiating ridge. We're going to use, again, the name ridge regression, two underscores, and then we're going to pass the hyper-parameter Alpha. So we pass in our estimator as well as the dictionary of each one of the different values that we want to loop through, and then we say we want to be, again, looking at holdout sets, how are we going to split up our data-set? We're going to use that k folds object that we defined earlier. As I mentioned, when we were doing cross val predict, you can also pass in a number if you don't have a k folds object created, but we create our grid objects."]},{"cell_type":"code","execution_count":48,"id":"98c0d6f5-d3c1-4ecd-8a33-154d9ea06972","metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# Same estimator as before\n","estimator = Pipeline([(\"polynomial_features\", PolynomialFeatures()),\n","        (\"scaler\", StandardScaler()),\n","        (\"ridge_regression\", Ridge())])\n","\n","params = {\n","    'polynomial_features__degree': [1, 2, 3],\n","    'ridge_regression__alpha': np.geomspace(4, 20, 30)\n","}\n","\n","grid = GridSearchCV(estimator, params, cv=kf)"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have our grid objects, that grid object is similar to many of the sklearn objects that we've been using so far or it's going to have this fit method and it's going to fit that pipeline, but running through each one of these different hyper-parameters. So we fit it, and that's going to take a bit a minute as it's going through 90 different hyper-parameters. It's going to figure out what the best score and the best parameters were. We are going to print that out and that's going to be, the best score they got was 0.85 and the best degree was degree two with a ridge regression Alpha of 4.0."]},{"cell_type":"code","execution_count":49,"id":"14d96e07-c899-406c-b4f2-ac2070c1b13b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Victo\\IBM-machine-learning-certification\\.conda\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n","  _data = np.array(data, dtype=dtype, copy=copy,\n"]},{"data":{"text/html":["<style>#sk-container-id-2 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-2 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-2 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-2 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-2 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-2 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-2 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-2 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-2 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-2 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-2 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-2 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-2 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-2 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-2 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-2 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-2 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=KFold(n_splits=3, random_state=72018, shuffle=True),\n","             estimator=Pipeline(steps=[(&#x27;polynomial_features&#x27;,\n","                                        PolynomialFeatures()),\n","                                       (&#x27;scaler&#x27;, StandardScaler()),\n","                                       (&#x27;ridge_regression&#x27;, Ridge())]),\n","             param_grid={&#x27;polynomial_features__degree&#x27;: [1, 2, 3],\n","                         &#x27;ridge_regression__alpha&#x27;: array([ 4.        ,  4.22826702,  4.46956049,  4.7246238 ,  4.99424274,\n","        5.27924796,  5.58051751,  5.89897953,  6.23561514,  6.59146146,\n","        6.96761476,  7.36523392,  7.78554391,  8.22983963,  8.69948987,\n","        9.19594151,  9.72072404, 10.27545421, 10.86184103, 11.48169104,\n","       12.13691388, 12.82952815, 13.56166768, 14.33558803, 15.15367351,\n","       16.01844446, 16.93256509, 17.89885162, 18.92028098, 20.        ])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=KFold(n_splits=3, random_state=72018, shuffle=True),\n","             estimator=Pipeline(steps=[(&#x27;polynomial_features&#x27;,\n","                                        PolynomialFeatures()),\n","                                       (&#x27;scaler&#x27;, StandardScaler()),\n","                                       (&#x27;ridge_regression&#x27;, Ridge())]),\n","             param_grid={&#x27;polynomial_features__degree&#x27;: [1, 2, 3],\n","                         &#x27;ridge_regression__alpha&#x27;: array([ 4.        ,  4.22826702,  4.46956049,  4.7246238 ,  4.99424274,\n","        5.27924796,  5.58051751,  5.89897953,  6.23561514,  6.59146146,\n","        6.96761476,  7.36523392,  7.78554391,  8.22983963,  8.69948987,\n","        9.19594151,  9.72072404, 10.27545421, 10.86184103, 11.48169104,\n","       12.13691388, 12.82952815, 13.56166768, 14.33558803, 15.15367351,\n","       16.01844446, 16.93256509, 17.89885162, 18.92028098, 20.        ])})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;polynomial_features&#x27;, PolynomialFeatures()),\n","                (&#x27;scaler&#x27;, StandardScaler()),\n","                (&#x27;ridge_regression&#x27;, Ridge(alpha=np.float64(4.0)))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;PolynomialFeatures<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\">?<span>Documentation for PolynomialFeatures</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>PolynomialFeatures()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Ridge<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html\">?<span>Documentation for Ridge</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>Ridge(alpha=np.float64(4.0))</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"],"text/plain":["GridSearchCV(cv=KFold(n_splits=3, random_state=72018, shuffle=True),\n","             estimator=Pipeline(steps=[('polynomial_features',\n","                                        PolynomialFeatures()),\n","                                       ('scaler', StandardScaler()),\n","                                       ('ridge_regression', Ridge())]),\n","             param_grid={'polynomial_features__degree': [1, 2, 3],\n","                         'ridge_regression__alpha': array([ 4.        ,  4.22826702,  4.46956049,  4.7246238 ,  4.99424274,\n","        5.27924796,  5.58051751,  5.89897953,  6.23561514,  6.59146146,\n","        6.96761476,  7.36523392,  7.78554391,  8.22983963,  8.69948987,\n","        9.19594151,  9.72072404, 10.27545421, 10.86184103, 11.48169104,\n","       12.13691388, 12.82952815, 13.56166768, 14.33558803, 15.15367351,\n","       16.01844446, 16.93256509, 17.89885162, 18.92028098, 20.        ])})"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["grid.fit(X, y)"]},{"cell_type":"code","execution_count":50,"id":"e00af536-b0eb-496e-82e7-0aa5f92e20c4","metadata":{},"outputs":[{"data":{"text/plain":["(np.float64(0.8546333782070435),\n"," {'polynomial_features__degree': 2,\n","  'ridge_regression__alpha': np.float64(4.0)})"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["grid.best_score_, grid.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["We can then actually use that grid in order to predict the x-values. Recall we were not able to do that with cross val predicts. The reason why we are able to do that now is that with grid search, it's going to test against all of the different holdout sets, and then once it's tested against all of the different holdout sets and it finds the best hyper-parameters that will fit for generalization for data-sets that we've never seen. It's then going to use those hyper-parameters to learn the parameters on the entire data-set because the more data that you have, the better you're going to be able to predict, and the idea is that you are now able to use this to predict as new data comes in. It's ready for production. So we're going to run grid.predicts since it's now fit on the entire data-set. Again, we don't have actual holdout data or new data, so it should perform fairly well, and we get an r2 score of 0.89."]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["y_predict = grid.predict(X)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"text/plain":["0.89647077145581"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["# This includes both in-sample and out-of-sample\n","r2_score(y, y_predict)"]},{"cell_type":"markdown","metadata":{},"source":["Then with the grid, we were going to call out our best estimator, that's an attribute that's available within the grid if we want to look at different attributes within our actual estimator that we're looking at. So we have our estimator, we do grid.estimator, best estimator is specifying the estimator that we had passed into our Grid Search CV. Then from there, we can pull out name steps such as ridge regression and get our coefficients."]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["array([ 0.00000000e+00,  2.47147038e-01, -1.18377806e+00,  1.11700133e+00,\n","        1.02268931e+00,  1.11573592e+00,  2.63515343e+00,  1.60400844e+00,\n","       -2.44378403e+00,  2.67549673e+00,  1.18120641e+00,  3.00931414e-01,\n","        7.17164316e-01,  5.46697603e-01,  1.16368605e+00,  2.78583120e-01,\n","        4.15770986e-01,  2.45396270e+00, -1.07012228e+00, -8.15812777e-01,\n","       -2.95036466e-01, -8.26480829e-01, -4.23552596e-01, -4.33022591e-02,\n","        8.34930099e-02, -4.92838202e-01,  5.12447325e-01,  5.88355149e-01,\n","       -3.50662224e-01, -1.39184102e-01, -2.99737738e-01,  1.02696466e+00,\n","        1.47705864e-02, -6.87593891e-01, -3.12176710e-01,  1.26476807e+00,\n","        1.01258007e+00, -5.51925730e-01, -8.62269698e-01,  8.78832853e-01,\n","        6.75975245e-02,  5.91362874e-01, -1.45086000e+00,  1.09286634e+00,\n","       -7.69112663e-01,  1.16978387e+00,  1.29460196e+00, -7.36888615e-01,\n","        7.29205860e-01, -2.44086792e+00,  1.02268931e+00, -2.26162754e+00,\n","       -2.56091292e+00,  7.93445821e-01,  8.93650928e-01, -1.40176885e+00,\n","       -5.90308431e-01,  6.48531603e-01,  1.51647270e+00, -6.34275596e-01,\n","       -2.70133478e-01, -1.21405240e+00, -7.39233780e-01, -1.08452081e+00,\n","       -5.78990020e-01, -7.37795974e-01, -1.12672094e+00,  2.99935516e-01,\n","        2.82902653e-01,  5.69116051e+00, -1.25149671e+00,  4.71964702e-01,\n","       -2.26642600e+00, -3.64722981e+00, -2.98653723e+00,  1.61479424e+00,\n","       -3.45481774e+00,  1.74801027e-01,  1.85117503e-05,  1.67217459e+00,\n","        2.86547168e-01, -7.33885392e-02, -1.23850226e+00, -1.78673478e+00,\n","        2.25040443e+00,  4.28050229e-01, -1.14432376e+00,  1.09490308e-01,\n","       -1.11626912e+00,  1.53508126e+00, -6.30486265e-01,  6.70247233e-01,\n","        2.24813158e+00,  3.12190879e-01, -2.71959257e+00,  2.18644985e-02,\n","        1.81340123e+00, -6.61376315e-02, -2.14156164e+00,  8.38822098e-01,\n","        3.11177198e-01,  2.69570504e-01, -6.64708129e-01, -1.41443974e+00,\n","        4.63455327e+00])"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["# Notice that \"grid\" is a fit object!\n","# We can use grid.predict(X_test) to get brand new predictions!\n","grid.best_estimator_.named_steps['ridge_regression'].coef_"]},{"cell_type":"markdown","metadata":{},"source":["Then if you do grid.cv results, this is actually a little bit cleaner if we do data-frame, and you can see for each one, if you want all the details of what grid search did, we are going through each one of the parameters, what the different scores were for each one of the splits. Then it looks at the mean score, the standard deviation of each one of those scores, and ultimately, picks the best mean score. So if you want to look at more details, you can look at the CV results."]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_polynomial_features__degree</th>\n","      <th>param_ridge_regression__alpha</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.003336</td>\n","      <td>4.742366e-04</td>\n","      <td>0.001333</td>\n","      <td>4.705275e-04</td>\n","      <td>1</td>\n","      <td>4.000000</td>\n","      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n","      <td>0.672111</td>\n","      <td>0.748235</td>\n","      <td>0.701801</td>\n","      <td>0.707382</td>\n","      <td>0.031327</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.003000</td>\n","      <td>3.371748e-07</td>\n","      <td>0.001000</td>\n","      <td>1.946680e-07</td>\n","      <td>1</td>\n","      <td>4.228267</td>\n","      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n","      <td>0.672103</td>\n","      <td>0.748207</td>\n","      <td>0.701986</td>\n","      <td>0.707432</td>\n","      <td>0.031307</td>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.002333</td>\n","      <td>4.717075e-04</td>\n","      <td>0.001001</td>\n","      <td>5.619580e-07</td>\n","      <td>1</td>\n","      <td>4.469560</td>\n","      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n","      <td>0.672093</td>\n","      <td>0.748175</td>\n","      <td>0.702178</td>\n","      <td>0.707482</td>\n","      <td>0.031286</td>\n","      <td>88</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.001999</td>\n","      <td>5.947204e-07</td>\n","      <td>0.001000</td>\n","      <td>4.899036e-07</td>\n","      <td>1</td>\n","      <td>4.724624</td>\n","      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n","      <td>0.672081</td>\n","      <td>0.748141</td>\n","      <td>0.702375</td>\n","      <td>0.707533</td>\n","      <td>0.031265</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.001999</td>\n","      <td>2.973602e-07</td>\n","      <td>0.001001</td>\n","      <td>1.123916e-07</td>\n","      <td>1</td>\n","      <td>4.994243</td>\n","      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n","      <td>0.672067</td>\n","      <td>0.748104</td>\n","      <td>0.702579</td>\n","      <td>0.707583</td>\n","      <td>0.031243</td>\n","      <td>86</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>85</th>\n","      <td>0.014300</td>\n","      <td>2.511614e-03</td>\n","      <td>0.002700</td>\n","      <td>4.936377e-04</td>\n","      <td>3</td>\n","      <td>16.018444</td>\n","      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n","      <td>0.827782</td>\n","      <td>0.867759</td>\n","      <td>0.859693</td>\n","      <td>0.851745</td>\n","      <td>0.017261</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>0.012660</td>\n","      <td>4.880183e-04</td>\n","      <td>0.002988</td>\n","      <td>8.173063e-04</td>\n","      <td>3</td>\n","      <td>16.932565</td>\n","      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n","      <td>0.827717</td>\n","      <td>0.867351</td>\n","      <td>0.859455</td>\n","      <td>0.851508</td>\n","      <td>0.017129</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>87</th>\n","      <td>0.013000</td>\n","      <td>3.421194e-05</td>\n","      <td>0.003015</td>\n","      <td>4.227613e-05</td>\n","      <td>3</td>\n","      <td>17.898852</td>\n","      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n","      <td>0.827628</td>\n","      <td>0.866926</td>\n","      <td>0.859179</td>\n","      <td>0.851244</td>\n","      <td>0.016996</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>0.013700</td>\n","      <td>2.035701e-03</td>\n","      <td>0.003299</td>\n","      <td>4.536990e-04</td>\n","      <td>3</td>\n","      <td>18.920281</td>\n","      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n","      <td>0.827515</td>\n","      <td>0.866483</td>\n","      <td>0.858866</td>\n","      <td>0.850955</td>\n","      <td>0.016863</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>89</th>\n","      <td>0.025381</td>\n","      <td>6.237030e-03</td>\n","      <td>0.003013</td>\n","      <td>8.247113e-04</td>\n","      <td>3</td>\n","      <td>20.000000</td>\n","      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n","      <td>0.827380</td>\n","      <td>0.866023</td>\n","      <td>0.858513</td>\n","      <td>0.850639</td>\n","      <td>0.016730</td>\n","      <td>32</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>90 rows × 13 columns</p>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n","0        0.003336  4.742366e-04         0.001333    4.705275e-04   \n","1        0.003000  3.371748e-07         0.001000    1.946680e-07   \n","2        0.002333  4.717075e-04         0.001001    5.619580e-07   \n","3        0.001999  5.947204e-07         0.001000    4.899036e-07   \n","4        0.001999  2.973602e-07         0.001001    1.123916e-07   \n","..            ...           ...              ...             ...   \n","85       0.014300  2.511614e-03         0.002700    4.936377e-04   \n","86       0.012660  4.880183e-04         0.002988    8.173063e-04   \n","87       0.013000  3.421194e-05         0.003015    4.227613e-05   \n","88       0.013700  2.035701e-03         0.003299    4.536990e-04   \n","89       0.025381  6.237030e-03         0.003013    8.247113e-04   \n","\n","    param_polynomial_features__degree  param_ridge_regression__alpha  \\\n","0                                   1                       4.000000   \n","1                                   1                       4.228267   \n","2                                   1                       4.469560   \n","3                                   1                       4.724624   \n","4                                   1                       4.994243   \n","..                                ...                            ...   \n","85                                  3                      16.018444   \n","86                                  3                      16.932565   \n","87                                  3                      17.898852   \n","88                                  3                      18.920281   \n","89                                  3                      20.000000   \n","\n","                                               params  split0_test_score  \\\n","0   {'polynomial_features__degree': 1, 'ridge_regr...           0.672111   \n","1   {'polynomial_features__degree': 1, 'ridge_regr...           0.672103   \n","2   {'polynomial_features__degree': 1, 'ridge_regr...           0.672093   \n","3   {'polynomial_features__degree': 1, 'ridge_regr...           0.672081   \n","4   {'polynomial_features__degree': 1, 'ridge_regr...           0.672067   \n","..                                                ...                ...   \n","85  {'polynomial_features__degree': 3, 'ridge_regr...           0.827782   \n","86  {'polynomial_features__degree': 3, 'ridge_regr...           0.827717   \n","87  {'polynomial_features__degree': 3, 'ridge_regr...           0.827628   \n","88  {'polynomial_features__degree': 3, 'ridge_regr...           0.827515   \n","89  {'polynomial_features__degree': 3, 'ridge_regr...           0.827380   \n","\n","    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n","0            0.748235           0.701801         0.707382        0.031327   \n","1            0.748207           0.701986         0.707432        0.031307   \n","2            0.748175           0.702178         0.707482        0.031286   \n","3            0.748141           0.702375         0.707533        0.031265   \n","4            0.748104           0.702579         0.707583        0.031243   \n","..                ...                ...              ...             ...   \n","85           0.867759           0.859693         0.851745        0.017261   \n","86           0.867351           0.859455         0.851508        0.017129   \n","87           0.866926           0.859179         0.851244        0.016996   \n","88           0.866483           0.858866         0.850955        0.016863   \n","89           0.866023           0.858513         0.850639        0.016730   \n","\n","    rank_test_score  \n","0                90  \n","1                89  \n","2                88  \n","3                87  \n","4                86  \n","..              ...  \n","85               23  \n","86               25  \n","87               27  \n","88               30  \n","89               32  \n","\n","[90 rows x 13 columns]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(grid.cv_results_)"]},{"cell_type":"code","execution_count":55,"id":"fb7c6636-9036-4126-bd04-fa2c80d72353","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8480525388350217\n"]},{"data":{"text/plain":["0.8667029116056759"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import (StandardScaler, \n","                                   PolynomialFeatures)\n","from scipy.stats.mstats import normaltest\n","from scipy.stats import boxcox\n","from scipy.special import inv_boxcox\n","\n","file_name='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ST0151EN-SkillsNetwork/labs/boston_housing.csv'\n","boston_data = pd.read_csv(file_name)\n","\n","lr = LinearRegression()\n","y_col = \"MEDV\"\n","X = boston_data.drop(y_col, axis=1)\n","y = boston_data[y_col]\n","\n","pf = PolynomialFeatures(degree=2, include_bias=False)\n","X_pf = pf.fit_transform(X)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3, \n","                                                    random_state=72018)\n","\n","s = StandardScaler()\n","X_train_s = s.fit_transform(X_train)\n","\n","bc_result = boxcox(y_train)\n","y_train_bc = bc_result[0]\n","lam = bc_result[1]\n","\n","lr.fit(X_train_s, y_train_bc)\n","X_test_s = s.transform(X_test)\n","y_pred_bc = lr.predict(X_test_s)\n","\n","y_pred_tran = inv_boxcox(y_pred_bc, lam)\n","print(r2_score(y_pred_tran,y_test)) #RES 0.848052537981275\n","\n","lr = LinearRegression()\n","lr.fit(X_train_s,y_train)\n","lr_pred = lr.predict(X_test_s)\n","r2_score(lr_pred,y_test) #RES 0.8667029116056716\n"]},{"cell_type":"code","execution_count":56,"id":"c135839d-98e5-466f-83b2-efae370cd0da","metadata":{},"outputs":[{"ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Unnamed: 0\nFeature names seen at fit time, yet now missing:\n- B\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Victo\\IBM-machine-learning-certification\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:597\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    596\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Victo\\IBM-machine-learning-certification\\.conda\\Lib\\site-packages\\sklearn\\pipeline.py:600\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Victo\\IBM-machine-learning-certification\\.conda\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n","File \u001b[1;32mc:\\Users\\Victo\\IBM-machine-learning-certification\\.conda\\Lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py:433\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform data to polynomial features.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    `csr_matrix`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 433\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    438\u001b[0m max_int32 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax\n","File \u001b[1;32mc:\\Users\\Victo\\IBM-machine-learning-certification\\.conda\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n","File \u001b[1;32mc:\\Users\\Victo\\IBM-machine-learning-certification\\.conda\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n","\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Unnamed: 0\nFeature names seen at fit time, yet now missing:\n- B\n"]}],"source":["y_predict = grid.predict(X)"]},{"cell_type":"code","execution_count":null,"id":"2646b1f8-ce1d-4f3e-840b-b49886cdeb34","metadata":{},"outputs":[],"source":["# This includes both in-sample and out-of-sample\n","r2_score(y, y_predict)"]},{"cell_type":"code","execution_count":null,"id":"1f1cc668-7e6b-47e8-a702-c28d83796b37","metadata":{},"outputs":[],"source":["# Notice that \"grid\" is a fit object!\n","# We can use grid.predict(X_test) to get brand new predictions!\n","grid.best_estimator_.named_steps['ridge_regression'].coef_"]},{"cell_type":"code","execution_count":null,"id":"871853a2-780b-4ea7-ace8-3a9aa1dc6833","metadata":{},"outputs":[],"source":["grid.cv_results_"]},{"cell_type":"markdown","id":"b08e2e4b-f86f-4b33-967f-126333baa281","metadata":{},"source":["## Summary\n","\n","In conclusion, in our notebook, we learned first just how to do those general K folds. We then moved on to using cross val predicts so that we can see the scores for each one of those K folds in assessing fashion. Then we talked about how we can use hyper-parameters in order to look at multiple versions of the same model, reducing or increasing the complexity of that model to see which one performs best on our holdout set. We passed that then using grid search CV. That's going to find that the best hyper-parameters, given the hyper-parameters that we gave it the options to run such as degree 1, 2, or 3, or Alphas within this range of 4-20. Then it will ultimately fit on all the data so that you will have the best hyper-parameters which are chosen and then tested across each one, as well as all of the best parameters as they will be learned on the entire data-set.\n","\n","1. We can manually generate folds by using `KFolds`\n","2. We can get a score using `cross_val_predict(X, y, cv=KFoldObject_or_integer)`. \n","   This will produce the out-of-bag prediction for each row.\n","3. When doing hyperparameter selection, we should be optimizing on out-of-bag scores. This means either using `cross_val_predict` in a loop, or ....\n","4. .... use `GridSearchCV`. GridSearchCV takes a model (or pipeline) and a dictionary of parameters to scan over. It finds the hyperparameter set that has the best out-of-sample score on all the parameters, and calls that it's \"best estimator\". It then retrains on all data with the \"best\" hyper-parameters.\n","\n","### Extensions\n","\n","Here are some additional items to keep in mind:\n","* There is a `RandomSearchCV` that tries random combination of model parameters. This can be helpful if you have a prohibitive number of combinations to test them all exhaustively.\n","* KFolds will randomly select rows to be in the training and test folds. There are other methods (such as `StratifiedKFolds` and `GroupKFold`, which are useful when you need more control over how the data is split (e.g. to prevent data leakage). You can create these specialized objects and pass them to the `cv` argument of `GridSearchCV`.\n"]},{"cell_type":"markdown","id":"49e1880e-cdc9-4019-9855-f4c6372f8ef8","metadata":{},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
