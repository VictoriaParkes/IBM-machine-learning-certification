{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e36f7b-b6d8-4f65-bd4a-09670e38f0ba",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Section 2, Part e: Regularization LAB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010ca53-4048-4d9e-8ef9-080bce940049",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "*   Implement data standardization\n",
    "*   Implement variants of regularized regression\n",
    "*   Combine data standardization with the train-test split procedure\n",
    "*   Implement regularization to prevent overfitting in regression problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759fcdf6-d61b-4203-b266-421119b465f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import piplite\n",
    "# await piplite.install(['tqdm', 'seaborn', 'pandas', 'numpy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5540b11b-3825-4267-892d-215379666abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wget\n",
    "import wget\n",
    "\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Surpress warnings:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e6581-ec2f-4892-83be-e19f461d3dcf",
   "metadata": {},
   "source": [
    "In the following cell we load the data and define some useful plotting functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e43f428-c1f0-4052-a6fb-c589833b9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(72018)\n",
    "\n",
    "\n",
    "\n",
    "def to_2d(array):\n",
    "    return array.reshape(array.shape[0], -1)\n",
    "\n",
    "\n",
    "    \n",
    "def plot_exponential_data():\n",
    "    data = np.exp(np.random.normal(size=1000))\n",
    "    plt.hist(data)\n",
    "    plt.show()\n",
    "    return data\n",
    "    \n",
    "def plot_square_normal_data():\n",
    "    data = np.square(np.random.normal(loc=5, size=1000))\n",
    "    plt.hist(data)\n",
    "    plt.show()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3955cf-d4f1-493b-aea1-abc9abfd78a3",
   "metadata": {},
   "source": [
    "### Loading in Boston Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc4d051-868d-4c69-a560-08acabcf9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyodide.http import pyfetch\n",
    " \n",
    "# async def download(url, filename):\n",
    "#     response = await pyfetch(url)\n",
    "#     if response.status == 200:\n",
    "#         with open(filename, \"wb\") as f:\n",
    "#             f.write(await response.bytes())\n",
    "# path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle\"\n",
    " \n",
    "#you will need to download the dataset; if you are running locally, please comment out the following \n",
    "# await download(path, \"boston_housing_clean.pickle\")\n",
    " \n",
    " \n",
    "# Import pandas library\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('boston_housing_clean.pickle')\n",
    "\n",
    "if not data_path.is_file():\n",
    "    url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle\"\n",
    "    downloaded_file = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18991d0a-8e69-407b-9334-b4ec91af6c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 rows of the dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('boston_housing_clean.pickle', 'rb') as to_read:\n",
    "    boston = pd.read_pickle(to_read)\n",
    "boston_data = boston['dataframe']\n",
    "boston_description = boston['description']\n",
    "\n",
    "# show the first 5 rows using dataframe.head() method\n",
    "print(\"The first 5 rows of the dataframe\") \n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0a4fb-4601-4eb0-83b6-f571f485c14a",
   "metadata": {},
   "source": [
    "## Data standardization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438cbe2-c798-4e4a-9385-9b6bb49447af",
   "metadata": {},
   "source": [
    "**Standardizing** data refers to transforming each variable so that it more closely follows a **standard** normal distribution, with mean 0 and standard deviation 1.\n",
    "\n",
    "So in regards to standardization, we know that standardizing data refers to transforming each variable so that is more closely following a standard normal distribution. That standard normal distribution will also have a mean of zero, and a standard deviation of one. So we'll be subtracting the mean, and dividing by the standard deviation.\n",
    "\n",
    "The [`StandardScaler`](http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01#sklearn.preprocessing.StandardScaler) object in SciKit Learn can do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ced5cc-0858-4d7e-8b10-b6bb3f3da216",
   "metadata": {},
   "source": [
    "**Generate X and y**:\n",
    "\n",
    "So first we're going to generate our x and y, our features and our outcome variable. We set y call equal to the string, MEDV, which is just the median value for each one of our households, and then the Boston data, we're going to drop this y column from our columns, and set that equal to x, x being our feature variables, and then y is going to be our outcome variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098d02a2-fbd9-4280-af2e-4dd6be74ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"MEDV\"\n",
    "\n",
    "X = boston_data.drop(y_col, axis=1)\n",
    "y = boston_data[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde772a-1c8a-46ef-9aa2-651851d3c202",
   "metadata": {},
   "source": [
    "**Import, fit, and transform using `StandardScaler`**\n",
    "\n",
    "We're then going to import our standard scaler and then quickly do our fit transform on our x data here. So now we started off with our object as we always do, s is equal to our standard scaler, and then we can call fit transform on our x. Now, I want you to be thinking, obviously, if we were going to have a train and test split, we'd do this a bit differently, and we'll discuss that in a couple more cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeff81b7-d86f-440d-9ee9-4f05ad2a4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "s = StandardScaler()\n",
    "X_ss = s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cb3c6-8ec7-4232-9751-22e678dd79c7",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "**Confirm standard scaling**\n",
    "\n",
    "So first, we want to confirm that the standard scaling is working appropriately. The way that we want to do this is to have you work using NumPy in order to recreate the standardization on your own. The hints that we add here is that if you had your array, which is now going to be two rows and three columns, we can call a.mean, and if we specify an axis, we can get the mean value across either all of the columns or all of the rows. Because we are trying to standardize each column, we obviously want to be working with the mean across all of the columns, we also want to be getting the standard deviation across each one of the columns, and then we can subtract the mean using this.mean method that's available for a NumPy arrays as well as the.std method, which is available with NumPy arrays as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef3a251-55b4-477b-9b70-4fb69f283af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "#Hint:\n",
    "\n",
    "a = np.array([[1, 2, 3], \n",
    "              [4, 5, 6]]) \n",
    "print(a) # 2 rows, 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fccc1e21-3846-47af-96d3-14a6df76a20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5, 3.5, 4.5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(axis=0) # mean along the *columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7e0232-2dc5-44e5-a9c0-2ea9da15e111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 5.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(axis=1) # mean along the *rows*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set X2 equal to just the array version of our x, which is our Pandas dataframe with all of our features, we're going to do a manual transformation, we'll call that man_transform. With that, we're going to do X2, which is just going to be our new NumPy array, which is all of our features, subtract the mean with the axis equal to zero. So looking back to the hint, we'd be essentially subtracting each one of our different mean values for that column, and then dividing by the standard deviation for that column. That should be all that your standard scaler should be doing under the hood. We're then going to check that the solution of this will end up being equal to one another. X_ss being this standard scaled version of our original dataframe that we have above using the standard scaler and man_transform is our new array where we created that manual transformation of the standard scaler on our own. The np.allclose is just telling us that every single value in two arrays are very similar to one another. By very similar, we mean that there may be under the hood some type of small difference due to just rounding error. So it would be something like the tenth decimal point can be difference and that's what the numpy.allclose. That'll be useful as we use different means of coming up with the same solution and trying to ensure that they are all the same values, all of the outcomes of the same values. So we call np.allclose and we see that that is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8ee371-8753-47e1-a384-e6a10e73913a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "X2 = np.array(X)\n",
    "man_transform = (X2-X2.mean(axis=0))/X2.std(axis=0)\n",
    "np.allclose(man_transform, X_ss)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338366d-af1a-4cd0-b9cf-ff323929acb6",
   "metadata": {},
   "source": [
    "### Coefficients with and without scaling\n",
    "\n",
    "Now that we've scaled our data and we have the scaled the non-scaled version, I am going to go in and create a linear regression object and fit our model to both our standardized and non-standardized version of our x data. So first we initiate our linear regression objects. We have our x and our y variables again, initiated to their original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bdcc4f0-6efa-4a72-9be1-bc400e7601a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96192e6-c7d9-4de9-8c3a-fca73b9fc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "y_col = \"MEDV\"\n",
    "\n",
    "X = boston_data.drop(y_col, axis=1)\n",
    "y = boston_data[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can just call lr.fit, lr being our linear regression object. We're fitting it on our x and our y, and then once we do that, we can see the coefficients that it has learned. We see that the coefficients are wildly different scales. We have the question underneath, is this a bad thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e22863-af2e-4567-bc4e-022d2e3516ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.107   0.046   0.021   2.689 -17.796   3.805   0.001  -1.476   0.306\n",
      "  -0.012  -0.953   0.009  -0.525]\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X, y)\n",
    "print(lr.coef_) # min = -18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7b6a6-1c40-44ef-a2f1-e28c7651dfbe",
   "metadata": {},
   "source": [
    "#### Discussion (together):\n",
    "\n",
    "The coefficients are on widely different scales. Is this \"bad\"?\n",
    "\n",
    "It's not necessarily a bad thing. So in regards to interpretability, in regards to how much will one unit change in each one of these columns change the outcome variable, this is much more interpretable. So one unit change in whatever this coefficient is. The first one, we'll subtract 0.108 from our median value households. So in that regards, it's much more interpretable. But in regards to figuring out which one of these different coefficients has the largest effect on the outcome of our median value, which one's the most important feature? We're not able to see that because that is now dependent, because each one of these coefficients are dependent on this scale of those original features, as we've discussed in prior lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Our Data\n",
    "\n",
    "So now let's scale are data, as we did before we do our standard scaler, x_ss is just our transform version of our features. We create a new linear regression object, we'll call it lr2, and then what do we print these out, we're able to see each one of these different coefficients. Now they are all on the same scale. So now a larger value means that it has a larger positive or negative, whatever it is, but a larger impact on our actual outcome variable. So as one of these features varies by a single standard deviation, we see how largely that will affect our median value households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "525f037d-9d42-493d-a91c-a8ca338e79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db69ef0-933b-4fa2-b2ad-f93e9e42db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X_ss = s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3bc66d4-88c6-44ad-a538-bb5b980461b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.92   1.081  0.143  0.682 -2.06   2.671  0.021 -3.104  2.659 -2.076\n",
      " -2.062  0.857 -3.749]\n"
     ]
    }
   ],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_ss, y)\n",
    "print(lr2.coef_) # coefficients now \"on the same scale\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf75119-afa9-435a-b001-f66a79add718",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Based on these results, what is the most \"impactful\" feature (this is intended to be slightly ambiguous)? \"In what direction\" does it affect \"y\"?\n",
    "\n",
    "**Hint:** Recall from last week that we can \"zip up\" the names of the features of a DataFrame `df` with a model `model` fitted on that DataFrame using:\n",
    "\n",
    "```python\n",
    "dict(zip(df.columns.values, model.coef_))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now using what we have here, we want to find the most impactful feature. All we're going to do is zip together our columns and our coefficients, as we've noticed so far, our coefficients are not coming with the names of each one of those coefficients. So what we're going to want to do is zip together the x.columns and the coefficients themselves so that they're aligned one with the other, which we'll see in a second, and then we're going to put this into a dataframe. So we see here that LSTAT, which is just lower status of the area is associated with the very lowest number and RM, which stands for the number of rooms, is associated with the highest coefficient, which makes sense. The more rooms we have, the higher our median value for our household would be, and the lower the status of the surrounding area, the lower median value of our household would be. That's the idea of getting the magnitude of each one of these coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec01f60-4472-464c-9b00-bb8b3ab44e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTAT</td>\n",
       "      <td>-3.748680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DIS</td>\n",
       "      <td>-3.104448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TAX</td>\n",
       "      <td>-2.075898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PTRATIO</td>\n",
       "      <td>-2.062156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOX</td>\n",
       "      <td>-2.060092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIM</td>\n",
       "      <td>-0.920411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AGE</td>\n",
       "      <td>0.021121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDUS</td>\n",
       "      <td>0.142967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAS</td>\n",
       "      <td>0.682203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B</td>\n",
       "      <td>0.856640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZN</td>\n",
       "      <td>1.080981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RAD</td>\n",
       "      <td>2.658787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RM</td>\n",
       "      <td>2.670641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "12    LSTAT -3.748680\n",
       "7       DIS -3.104448\n",
       "9       TAX -2.075898\n",
       "10  PTRATIO -2.062156\n",
       "4       NOX -2.060092\n",
       "0      CRIM -0.920411\n",
       "6       AGE  0.021121\n",
       "2     INDUS  0.142967\n",
       "3      CHAS  0.682203\n",
       "11        B  0.856640\n",
       "1        ZN  1.080981\n",
       "8       RAD  2.658787\n",
       "5        RM  2.670641"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "pd.DataFrame(zip(X.columns, lr2.coef_)).sort_values(by=1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50434075-ac60-4585-91bd-b697a3797866",
   "metadata": {},
   "source": [
    "Looking just at the strength of the standardized coefficients LSTAT, DIS, RM and RAD are all the 'most impactful'. Sklearn does not have built in statistical signifigance of each of these variables which would aid in making this claim stronger/weaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18ddd0-f80b-4b61-bf55-d324d2649013",
   "metadata": {},
   "source": [
    "### Lasso with and without scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3090197-0fb6-4a29-a7b6-0b048554bda0",
   "metadata": {},
   "source": [
    "We discussed Lasso in lecture.\n",
    "\n",
    "Let's review together:\n",
    "\n",
    "1.  What is different about Lasso vs. regular Linear Regression?\n",
    "\n",
    "It's all about the cost function where we are adding on to that cost function, the absolute value of the coefficients to limit the size of each one of those coefficients.\n",
    "\n",
    "Cost Function: Lasso adds a penalty to the cost function based on the absolute values of the coefficients, which helps limit their size.\n",
    "\n",
    "2.  Is standardization more or less important with Lasso vs. Linear Regression? Why?\n",
    "\n",
    "With linear regression, we won't have any penalization according to the size of our coefficients, whereas lasso we'll have extra penalty if those coefficients are larger versus them being smaller. So the scale of our features are going to affect the scale of our coefficients, and if we don't scale first and put them all on the same scale, then we end up having coefficients that are prone to the scale of each one of these features, and therefore can be larger or smaller, and therefore get penalized more or less according to the scale of each one of the features, so it's much more important to bring all of those features on the same scale.\n",
    "\n",
    "Standardization: It's crucial to standardize features before applying Lasso, as the scale of features affects the coefficients. Without standardization, coefficients can vary significantly, leading to inconsistent penalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c5e50b9-cc12-41df-bb37-afdf2fb905a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20c941-ab44-4302-8de1-af85744e3a75",
   "metadata": {},
   "source": [
    "#### Create polynomial features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3449ce-32d3-418f-9a51-e05421518a4e",
   "metadata": {},
   "source": [
    "[`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n",
    "\n",
    "We're going to initiate our polynomial features objects, our degree is equal to two, we've seen this before, we say that we don't want to include the bias, that's just saying that we don't want to include the intercept term, now just output a bunch of ones for that column, and we'd learned a coefficient for all those ones, but we don't need that, our lasso will take care of that automatically, and then we call fit transform on our original x to come up with our polynomial transformed version of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "651672b1-81e4-4541-a987-a704c6b8cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2, include_bias=False,)\n",
    "X_pf = pf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465f404-763a-419c-8aff-bb35e88f7fb2",
   "metadata": {},
   "source": [
    "**Note:** We use `include_bias=False` since `Lasso` includes a bias by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do our standardization by calling out that StandardScaler that we defined earlier as s and calling fit_transform on our polynomial features version of x, and we state that as x_pf_ss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "659a8141-be69-4ec7-bd83-09fa1bcff2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pf_ss = s.fit_transform(X_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b599f-3ff8-4178-898b-b7ab956fee38",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Model Fitting: The lecture discusses fitting a Lasso model using polynomial features and standard scaling. The default alpha value is set to 1.0, which results in high regularization and many coefficients being zeroed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2132f83-c673-4c10-8b40-6f5f6ddbb28f",
   "metadata": {},
   "source": [
    "[`Lasso` documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n",
    "\n",
    "We're then going to use the default version of lasso. So we're not going to pass in any hyper parameters and we set las equal to this default version of lasso, we call fit on our polynomial transform plus the standard scaled version of our x values with our y, and then we can see the coefficients that it comes up with. By defaults, the Alpha value is going to be 1.0 here. In this case, this is a fairly high Alpha value and it ends up zeroing out a ton of our coefficients. High Alpha value again, is going to mean more regularization, which means a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ca1723c-fa79-42dc-a9b5-0c493d1b6234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.   ,  0.   , -0.   ,  0.   , -0.   ,  0.   , -0.   , -0.   ,\n",
       "       -0.   , -0.   , -0.991,  0.   , -0.   , -0.   ,  0.   , -0.   ,\n",
       "        0.068, -0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n",
       "       -0.   , -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   , -0.   ,  0.   ,\n",
       "       -0.   , -0.   , -0.   , -0.05 , -0.   , -0.   , -0.   , -0.   ,\n",
       "       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n",
       "       -0.   , -0.   ,  0.   , -0.   ,  3.3  , -0.   , -0.   , -0.   ,\n",
       "       -0.   , -0.   ,  0.42 , -3.498, -0.   , -0.   , -0.   , -0.   ,\n",
       "       -0.   ,  0.   , -0.   , -0.   , -0.   , -0.146, -0.   , -0.   ,\n",
       "       -0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n",
       "       -0.   , -0.   , -0.   ,  0.   , -0.   ,  0.   , -0.   , -0.   ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las = Lasso()\n",
    "las.fit(X_pf_ss, y)\n",
    "las.coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f19b3-bee7-4125-be9d-d139b2f352da",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Compare\n",
    "\n",
    "*   Sum of magnitudes of the coefficients:\n",
    "\n",
    "    Now we want to compare the sum of the magnitude of our coefficients, which is just going to be the sum of the absolute values to see adding up all of the coefficients, how large is that sum going to be? The larger that is, the more complex our model is. \n",
    "\n",
    "*   Number of coefficients that are zero:\n",
    "\n",
    "    Then we're also going to look at the number of coefficients that are equal to zero to see how many of our terms are zeroed out.\n",
    "\n",
    "for Lasso with alpha 0.1 vs. 1.\n",
    "We're going to do this for lasso with Alpha equal to 0.1, which is a smaller Alpha, versus an Alpha of one, which is a larger Alpha, more regularization, and should therefore be a less complex model.\n",
    "\n",
    "Magnitude and Coefficients: The magnitude of coefficients and the count of non-zero coefficients are compared for different alpha values (0.1 vs. 1.0). Higher alpha leads to lower magnitude and more zeros.\n",
    "\n",
    "Before doing the exercise, answer the following questions in one sentence each:\n",
    "\n",
    "*   Which do you expect to have greater magnitude?\n",
    "*   Which do you expect to have more zeros?\n",
    "\n",
    "If the Alpha is larger, then we are regularizing more, so we'd have smaller magnitude for a higher Alpha, as well as more zeros for a higher Alpha, it will zero out more values if we have an higher Alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're setting the las 01, it's just going to be equal to Alpha, where the Alphas equal to 0.1, rather than the default of one, and then we're going to fit our polynomial transform StandardScaler version of our x as well as our y, and then we're going to get the absolute value of each one of the coefficients, and take the sum and that will give us the magnitude. Then we're also going to say for each one of our coefficients, how many of them are not equal to zero? They'll pass over true or false values that are the same size of the array, and then we sum that together, each true value will be equal to one, and when we sum that together, then we end up having the number of coefficients that are non-zero. You run this, and we see that we have a 23 coefficients that are non-zero and our magnitude is 26.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "659b3c6c-2062-4ffb-adac-a0bfea4b22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of coefficients: 26.17241511542676\n",
      "number of coefficients not equal to 0: 23\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "las01 = Lasso(alpha = 0.1)\n",
    "las01.fit(X_pf_ss, y)\n",
    "print('sum of coefficients:', abs(las01.coef_).sum() )\n",
    "print('number of coefficients not equal to 0:', (las01.coef_!=0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to pull this out to show you what this looks like, we ended up with all true or falses, and again, those trues would be equal to one, so when we called out sum, we get the number 23, and that's just the number of values that are greater than zero or not equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False,  True, False,  True, False,\n",
       "        True, False, False, False, False, False,  True,  True,  True,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False,  True,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False,  True, False,\n",
       "       False, False,  True,  True, False, False, False,  True, False,\n",
       "       False, False, False, False,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las01.coef_!=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set Alpha equal to one, so now we have a higher Alpha, more regularization, we run the same steps and as expected, we have a lower magnitude of 8.4 and less values that are not equal to zero, of only seven values that are not equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9344dd0-d4a7-48fe-8cb6-26e563d76347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of coefficients: 8.472405227760158\n",
      "number of coefficients not equal to 0: 7\n"
     ]
    }
   ],
   "source": [
    "las1 = Lasso(alpha = 1)\n",
    "las1.fit(X_pf_ss, y)\n",
    "print('sum of coefficients:',abs(las1.coef_).sum() )\n",
    "print('number of coefficients not equal to 0:',(las1.coef_!=0).sum())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380a6e6-8a4e-44a4-9df9-e68ab39a5489",
   "metadata": {},
   "source": [
    "More regularization again, meant lower magnitude as well as more zeros, less non-zero values in our coefficients. \n",
    "\n",
    "With more regularization (higher alpha) we will expect the penalty for higher weights to be greater and thus the coefficients to be pushed down. Thus a higher alpha means lower magnitude with more coefficients pushed down to 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb28fa-db14-4ada-a263-0c3ec1e866a0",
   "metadata": {},
   "source": [
    "### Exercise: $R^2$\n",
    "\n",
    "Model Performance: Regularization generally reduces model complexity, which can lower performance on the training set but is intended to improve performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a345c79-2628-4589-bbb9-f476590973be",
   "metadata": {},
   "source": [
    "Calculate the $R^2$ of each model without train/test split.\n",
    "\n",
    "Recall that we import $R^2$ using:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the r2_score of each one of our models. We're going to import r2_score, hence we are going to run r2_score on y, as well as our predicted value for x_pf_ss. This las is the las that was defined up here, which had the default of 1.0, which was a fairly high regularization, and we see that the r2_scores about 72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99bfec9e-ae1c-4f5a-b43a-bace4707fdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207000461229028"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y,las.predict(X_pf_ss))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b794ed-ec4d-4bd8-8bcb-d4a4d1337602",
   "metadata": {},
   "source": [
    "#### Discuss:\n",
    "\n",
    "Will regularization ever increase model performance if we evaluate on the same dataset that we trained on?\n",
    "\n",
    "The idea, again, of regularization is to reduce the complexity, ensure that we don't overfit to our training set. If we are ensuring that we're not overfitting to our training set, then we're reducing how well we can fit to the actual training set, therefore, if we're testing and saying how well we able to predict on the same set that we trained, we will always get a lower value for r if we increase regularization. The key with regularization is to see how well it will perform on holdout sets, on new sets coming in, which is generally going to be the problem that you're trying to solve. You'll never be trying to predict on the same values that you already have available to you. With that in mind, we want to introduce train_test_split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656a287-bdb2-4d21-ba25-20dcb127b407",
   "metadata": {},
   "source": [
    "## With train/test split\n",
    "\n",
    "Train-Test Split: The importance of splitting data into training and testing sets is emphasized, ensuring that scaling is only applied to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f2354-c05f-4d76-a528-9d5ff9de3237",
   "metadata": {},
   "source": [
    "#### Discuss\n",
    "\n",
    "Are there any issues with what we've done so far?\n",
    "\n",
    "**Hint:** Think about the way we have done feature scaling.\n",
    "\n",
    "Discuss in groups of two or three.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "424508ee-dd62-429e-8e5a-123c9336a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to call our train_test_split function, we're going to pass in x_pf and y, we're not calling in the scaled version, because if you recall, we can only scale on our training set, we can only fit our scaling to our training set, and then we can use that fit in order to transform our test set. So we're using the polynomial transform but not the StandardScaler transform, to get our x_train, x_test, y_train and y_test, we're holding out 30 percent for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "743c7c93-5775-433e-b783-27a5fbc5c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to run our fit_transform and fit. So we only fit_transform again, on our training set. We can then fit our last model that we defined on our new x_train standardize and our y_train. We're then going to just transform our x_test using the StandardScaler, which has been fit to our x_train, and then we can come up with our predictions on our x_test_s on our standardized version of x_test, and then we can see the r2_score for our holdout set. We run this and we see a fairly low r2_score of 0.33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b30ee35c-8bef-4279-9b41-82b1901dca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6780325981174931"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s = s.fit_transform(X_train)\n",
    "las.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred = las.predict(X_test_s)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the same steps for an Alpha of 0.1. We run this and we see an increased r2_score, which means that if we think between these two models, this one had high regularization, the model above had lower regularization. What would we think is the problem with the above dataset? The problem would be that we included too much regularization, we have too much bias, and we're not able to find the actual underlying relationship between x and y. We have dumbed down our model too much, we've reduced the complexity too much, and again, the goal is to find this balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d28256b-5de4-4e04-84e8-fc744be75580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.799926134284606"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s = s.fit_transform(X_train)\n",
    "las01.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred = las01.predict(X_test_s)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will try different values of Alpha to try to hone in on what the optimal value for Alpha will be in order to get a higher r2_score or a higher whatever our metric is in regards to our holdout set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d653792-74e9-4c0a-9077-cbe59ef1a91a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In this section, we're going to look at another Alpha value for Lasso. We're going to see how well that's going to perform on a holdout set, and then compare that to linear regression, check the r2 score for each and see which one has the higher r2 score, and then we will also look at in part 3, which one is going to have a smaller magnitude of coefficients, as well as which one is going to have less overall coefficients. Hopefully, already you have in mind that Lasso should be reducing the magnitude of these coefficients, as well as bringing down the number of non-zero coefficients that we have in our resulting model.\n",
    "\n",
    "#### Part 1:\n",
    "\n",
    "Do the same thing with Lasso of:\n",
    "\n",
    "*   `alpha` of 0.001\n",
    "*   Increase `max_iter` to 100000 to ensure convergence.\n",
    "\n",
    "Calculate the $R^2$ of the model.\n",
    "\n",
    "Feel free to copy-paste code from above, but write a one sentence comment above each line of code explaining why you're doing what you're doing.\n",
    "\n",
    "#### Part 2:\n",
    "\n",
    "Do the same procedure as before, but with Linear Regression.\n",
    "\n",
    "Calculate the $R^2$ of this model.\n",
    "\n",
    "#### Part 3:\n",
    "\n",
    "Compare the sums of the absolute values of the coefficients for both models, as well as the number of coefficients that are zero. Based on these measures, which model is a \"simpler\" description of the relationship between the features and the target?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "##### Part 1:\n",
    "So first, we're going to initiate our object, las001 is just going to be equal to a Lasso regression. Well, with an Alpha equal to 0.001, we are increasing the max iterations from its default. When we have a smaller Alpha value, it can happen that we are not able to get to the optimal solution. We briefly touched on the fact that Lasso takes longer because it uses something called gradient descent to step through until it gets to the optimal solution. So we want to ensure that we have enough steps to get to that optimal solution. Otherwise, it could end up stopping short of that optimal solution and not converging. When that happens, you will probably get a warning that you were not able to converge. We're going to set X_train_s to our fit transformed version of X_train using s. We're fit transforming it to the standard scaler. We're then going to take that X_train_s as well as our Y_train and we're going to fit our new Lasso regression to our training set. We're then going to transform our X_test. We're going to use s.transform, not fit_transform, but just transform and then we are going to use that as input to our prediction in order to get the predictions on our test set. We can then get the r2 score for our predictions versus our test set, and we'll print that out and you'll see that once we run all the code here.\n",
    "\n",
    "##### Part 2:\n",
    "Part 2 was to do the same thing for linear regression. So we initiate our linear regression objects, we fit it to our training set using our standardized version of X_train. We then predict using the standardized versions of the X_test that we defined in part 1 (X_train_s). Then we're going to print out the r2 score for our linear regression, which is just going to be the y_pred_lr, and our original outcome variable.\n",
    "\n",
    "##### Part 3:\n",
    "We're then also going to use the same code that we used above to see the magnitude of our coefficients by taking the absolute value and the sum of each one of those absolute values, and then we're going to see how many of our values are non-zero. So first we're going to do it for Lasso, then we're going to do it for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0997b196-6e14-437e-9226-7c70077deb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score for Lasso, alpha = 0.001: 0.8847893236874363\n",
      "r2 score for Linear Regression: 0.8689110469231008\n",
      "Magnitude of Lasso coefficients: 435.572322904404\n",
      "Number of coeffients not equal to 0 for Lasso: 90\n",
      "Magnitude of Linear Regression coefficients: 1183.8918138649396\n",
      "Number of coeffients not equal to 0 for Linear Regression: 104\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# Part 1\n",
    "\n",
    "# Decreasing regularization and ensuring convergence\n",
    "las001 = Lasso(alpha = 0.001, max_iter=100000)\n",
    "\n",
    "# Transforming training set to get standardized units\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "\n",
    "# Fitting model to training set\n",
    "las001.fit(X_train_s, y_train)\n",
    "\n",
    "# Transforming test set using the parameters defined from training set\n",
    "X_test_s = s.transform(X_test)\n",
    "\n",
    "# Finding prediction on test set\n",
    "y_pred = las001.predict(X_test_s)\n",
    "\n",
    "# Calculating r2 score\n",
    "print(\"r2 score for Lasso, alpha = 0.001:\", r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Part 2\n",
    "\n",
    "# Using vanilla Linear Regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fitting model to training set\n",
    "lr.fit(X_train_s, y_train)\n",
    "\n",
    "# predicting on test set\n",
    "y_pred_lr = lr.predict(X_test_s)\n",
    "\n",
    "# Calculating r2 score\n",
    "print(\"r2 score for Linear Regression:\", r2_score(y_test,y_pred_lr))\n",
    "\n",
    "\n",
    "# Part 3\n",
    "print('Magnitude of Lasso coefficients:', abs(las001.coef_).sum())\n",
    "print('Number of coeffients not equal to 0 for Lasso:', (las001.coef_!=0).sum())\n",
    "\n",
    "print('Magnitude of Linear Regression coefficients:', abs(lr.coef_).sum())\n",
    "print('Number of coeffients not equal to 0 for Linear Regression:', (lr.coef_!=0).sum())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's look at these results. It's iterating through, so it takes just a second and then we see the r2 score for Lasso was 0.868, and the r2 score for linear regression was 0.855. So on the holdout set, Lasso did a better job of explaining the additional variation. We then see the magnitude of the Lasso coefficients is 436 compared to the magnitude of the coefficients for linear regression, which was nearly triple that, at 1,185. Then we see that there is 89 non-zero coefficients and 104 coefficients for the linear regression. So we see how we are able to reduce the complexity of the model, reduce the number of coefficients, reduce the magnitude of those coefficients, and come up with a solution that's doing a better job of explaining variation on our holdout set. So there's that idea of bringing down the variance of our model in order for it to generalize better on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef4f00-7b20-46e8-9986-6d5bed2e9089",
   "metadata": {},
   "source": [
    "## L1 vs. L2 Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00225a7-835b-46c7-93ad-9d722b82d5d0",
   "metadata": {},
   "source": [
    "As mentioned in the deck: `Lasso` and `Ridge` regression have the same syntax in SciKit Learn.\n",
    "\n",
    "Now we're going to compare the results from Ridge vs. Lasso regression:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d14095-9525-4481-896b-9911b62be0b4",
   "metadata": {},
   "source": [
    "[`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42c1c1b0-017b-431f-bf92-9eadbfd103c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fff436-9f66-4d58-9ec2-6d48a813fbda",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now, let's quickly introduce the same concept using Ridge regression. We have gone through this before. We saw that Lasso and Ridge had a similar formulation so we're just going to import the Ridge regression object. We're then going to fit our Ridge to 0.001 using that same Alpha. Now that Alpha doesn't necessarily translate, so don't think that just because they have the same Alpha, they will have the same regularization again. Now we're regularizing on the coefficient squared. So it could have a different effect then we may want to loop through different values of Alpha. But here we're just going to look at 0.001. We're going to do the same fit_transform using our standard scalar. We're then going to fit our Ridge model, which is just r, to our X_train_s and our y_train. We have our X_test_s using the s.transform, and then we can come up with our predictions for using r on our test set.\n",
    "\n",
    "Following the Ridge documentation from above:\n",
    "\n",
    "1.  Define a Ridge object `r` with the same `alpha` as `las001`.\n",
    "2.  Fit that object on `X` and `y` and print out the resulting coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45d9a6c4-bb2e-48ba-adf1-adf5b03131d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.692,   9.889, -25.005,   5.31 ,  -2.61 ,  14.958,  22.314,\n",
       "       -22.858,  27.765,  -1.565,  17.077,  21.858,  11.574,   1.05 ,\n",
       "         0.431,  13.786,   1.814,  -8.345,   4.994,  -3.564,  -3.436,\n",
       "       -16.336,  -7.047,   6.609,  -1.477,   4.686,  -1.304,  -0.059,\n",
       "        -0.304, -12.842,   1.977,   1.081,  -0.676,  -1.079,   4.492,\n",
       "        -4.267,   4.675,  -1.281,   8.666,  -0.274,  -8.12 ,  11.783,\n",
       "         6.538,   1.323,   2.06 ,   0.899,   1.789,   4.744,  -4.664,\n",
       "         5.31 ,  -3.236,  -8.668,   0.973,   1.136,   0.29 ,  -1.631,\n",
       "        -2.926,   2.923,  -0.735,  11.896,   0.754,  -7.531,  18.306,\n",
       "       -22.168,  35.651, -22.381,  -7.378,   1.732,   4.068, -12.254,\n",
       "        -3.721,  -5.537, -16.169,  -5.872,  -4.515,  -9.903,   0.942,\n",
       "        -0.095,  17.18 , -14.268,  -2.904,  -2.824,  -5.418,  11.908,\n",
       "         0.065,  -4.433,  -5.679,  -3.515,   0.822, -29.082,  49.345,\n",
       "       -21.72 ,  -1.428,  -8.864, -15.682,  11.982,  -0.818,   2.924,\n",
       "        -0.701,  -4.375,   2.712,  -2.48 ,  -2.382,   1.707])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Decreasing regularization and ensuring convergence\n",
    "r = Ridge(alpha = 0.001)\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "r.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred_r = r.predict(X_test_s)\n",
    "\n",
    "# Calculating r2 score\n",
    "r.coef_\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have all of these coefficients for r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remind that we're working with the same Alpha as above. By just calling Lasso, we see the Alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7541493a-8598-4a29-945d-d2c47ca28a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=0.001, max_iter=100000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Lasso<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Lasso.html\">?<span>Documentation for Lasso</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Lasso(alpha=0.001, max_iter=100000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "Lasso(alpha=0.001, max_iter=100000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las001 # same alpha as Ridge above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can look at the Lasso coefficients, and we see that we have a ton more zeros. Again, those zeros are due to the fact that we are using Lasso. Lasso by default will almost always zero more values than Ridge will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8f9c9b1-212e-4c36-8585-195c2fb2cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.   ,   0.   , -17.008,   2.592,   0.   ,  13.431,  10.062,\n",
       "       -19.542,   9.237,   0.   ,   6.159,  17.087,  11.486,   1.208,\n",
       "         0.219,  10.827,   2.185,  -7.106,   4.312,  -1.526,  -2.092,\n",
       "        -9.669,   0.   ,   0.   ,  -1.177,   3.865,   0.378,   0.194,\n",
       "        -0.296,  -3.478,   0.291,   0.72 ,  -0.795,  -0.74 ,   2.407,\n",
       "        -0.892,   2.835,  -0.984,   3.864,  -0.963,   6.914,   6.2  ,\n",
       "         4.197,   0.892,   2.028,   2.608,  -3.975,   2.574,  -4.568,\n",
       "         2.086,  -2.   ,  -7.412,   1.604,   1.676,  -1.248,  -0.   ,\n",
       "        -0.   ,   2.576,  -0.851,   3.206,  -0.   ,  -2.429,   9.276,\n",
       "        -7.056,   0.   ,  -5.955,  -5.326,   0.531,   3.344,  -7.575,\n",
       "        -0.   ,  -6.762, -10.2  ,  -6.428,  -2.835,  -9.561,   0.288,\n",
       "         0.549,  11.094,  -6.731,  -1.161,  -2.301,  -3.555,   9.12 ,\n",
       "        -0.014,  -2.384,  -2.042,  -1.691,   0.798, -20.822,  27.09 ,\n",
       "         0.   ,  -1.527,  -6.793,  -5.084,  15.577,   0.   ,  -0.   ,\n",
       "         0.569,  -3.456,   2.458,  -2.501,  -2.223,   2.226])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las001.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to look at both the magnitude of our coefficients as well as the number that are zeroed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "701d0cff-0841-4ce8-b9f5-4369e2839591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792.867375512679\n",
      "435.572322904404\n",
      "104\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.abs(r.coef_)))\n",
    "print(np.sum(np.abs(las001.coef_)))\n",
    "\n",
    "print(np.sum(r.coef_ != 0))\n",
    "print(np.sum(las001.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected for a Ridge, we had more values. In fact, we didn't zero out any values compared to the number of values that we zeroed out or we have left that are nonzero for Lasso. In this particular version of Ridge and Lasso, we see that the magnitude was higher. We ran Ridge first, the magnitude was higher for Ridge and we had a simpler model for Lasso. So here Lasso is providing stronger overall regularization than Ridge for this particular Alpha. Again, this is not always necessarily true. It'll depend on your features. But we are able to test it using the methods that we see above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27121e4-c0a5-4edc-9ade-c163305f431a",
   "metadata": {},
   "source": [
    "**Conclusion:** Ridge does not make any coefficients 0. In addition, on this particular dataset, Lasso provides stronger overall regularization than Ridge for this value of `alpha` (not necessarily true in general).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then just going to look at r2 score for each. The first one is the r2 score for Ridge. The second one is the r2 score for our Lasso. Again, Lasso is doing a better job of generalizing, but this will obviously depend on the Alphas chosen, and you can iterate through different Alphas to see if certain values work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8f177df-182f-4264-8b74-266a3b78c265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9076091395029868\n",
      "0.9103503442034426\n"
     ]
    }
   ],
   "source": [
    "y_pred = r.predict(X_pf_ss)\n",
    "print(r2_score(y, y_pred))\n",
    "\n",
    "y_pred = las001.predict(X_pf_ss)\n",
    "print(r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292a818-cd0b-4497-96b0-8007d07eee8e",
   "metadata": {},
   "source": [
    "**Conclusion**: Ignoring issues of overfitting, Lasso does slightly better than Ridge when `alpha` is set to 0.001 for each (not necessarily true in general).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdefd75-b48f-4bfc-9839-f144df38aa58",
   "metadata": {},
   "source": [
    "# Example: Does it matter when you scale?\n",
    "\n",
    "Now, does it matter when you scale? So we've been honing in on the fact that first, and we've discussed why, but first you want to do your transformation on your training set, your fit and transform on your training set, then just transform on your test set. Here we're going to see if we did plane linear regression. So here we're actually taking a standard scaled version first then running linear regression, and we get the r2 score, 0.58, and then in this version we're doing it as we said, where we should do our fit_transform first, then r transform. So if we look here, this is already having the fit_transform done for us, and that's going to be the same values for both the train and test set. Here, we're only using the train set in order to come up with our mean and standard deviation rather than the full dataset. Then we transform our test set using those values. But we see here that we ended up with the same r2 score. The conclusion that we want to take away here that we want to hone in on is that yes, it will be the same when you're working with plain vanilla linear regression. But as soon as you work with other models where the coefficients will matter in our cost score and we introduce things such as regularization, we may end up with different r2 scores or different predictions given when we actually fit and transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eeae8d2-a1c7-4cc9-9994-5c143672d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3bee345-bf28-445a-aeef-8affefb8957e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6982083583132748"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab5b5dac-82bb-4f33-8b8c-080e58bb56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f32f69bf-c9ff-4d23-a2a0-50ba9df6daca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6982083583132748"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = StandardScaler()\n",
    "lr_s = LinearRegression()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "lr_s.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred_s = lr_s.predict(X_test_s)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to make this clear, imagine I just change this to a Lasso regression and I change this to Lasso regression, we'll see that using the entire dataset to come up with our standard deviation and our mean will come up with a different prediction than using just our training set to come up with those values. They'll be very similar because the mean and standard deviation should be fairly similar, but they will be different. So again, when you're working with most models, you want to run the fit_transform only on your training set, and then that transform on just your test set, not the fit transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6253107263912051"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Lasso()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6253107263912051"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = StandardScaler()\n",
    "lr_s = Lasso()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "lr_s.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred_s = lr_s.predict(X_test_s)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ef315-ca74-47ae-a20d-3a426f6dc294",
   "metadata": {},
   "source": [
    "**Conclusion:** It doesn't matter whether you scale before or afterwards, in terms of the raw predictions, for Linear Regression. However, it matters for other algorithms. Plus, as we'll see later, we can make scaling part of a `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7277c-58b5-4e05-bcbb-033a1a887e45",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "c04a462d0df8b96d65cb3d5a3fb3a825548b3726460be6f51dbd23591fe0c571"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
