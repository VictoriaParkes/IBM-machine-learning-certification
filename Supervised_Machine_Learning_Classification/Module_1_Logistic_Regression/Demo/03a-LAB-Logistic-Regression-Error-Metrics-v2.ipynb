{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c60a963-341f-458a-9bcd-58671c2d377b",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 3, Part a: Logistic Regression LAB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21dc06c-4b15-4e84-b4c8-c3567664cd0c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants who carried a smartphone with an embedded inertial sensor while performing activities of daily living (ADL). The objective is to classify the activities the participants performed into one of the six following categories: walking, walking upstairs, walking downstairs, sitting, standing, and laying.\n",
    "\n",
    "The following information is provided for each record in the dataset: \n",
    "\n",
    "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration \n",
    "- Triaxial Angular velocity from the gyroscope \n",
    "- A 561-feature vector with time and frequency domain variables \n",
    "- The activity label \n",
    "\n",
    "More information about the features are available on the website linked above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c1ad14-9fcc-4453-97cc-bec1c464ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from seaborn) (2.0.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\victo\\ibm-machine-learning-certification\\.conda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn  \n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install  matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7500a519-7e45-42f6-8c0f-33c13cac83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a650785-6dd0-44fe-b737-a000f8b97a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31edc2-cbee-4b1a-9634-75edd142e31e",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Import the data and do the following:\n",
    "\n",
    "* Examine the data types--there are many columns, so it might be wise to use value counts.\n",
    "* Determine if the floating point values need to be scaled.\n",
    "* Determine the breakdown of each activity.\n",
    "* Encode the activity label as an integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80572ef4-180e-465c-b5a8-9a8d513fe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Human_Activity_Recognition_Using_Smartphones_Data.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143ae98-68e0-447d-bccf-6b8fc469510c",
   "metadata": {},
   "source": [
    "The data columns are all floats except for the activity label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd45f3-b9e7-4de4-ab50-33190dc52f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622d61d-162f-405a-8442-cd8887322393",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a02e78-d20c-406e-899d-35c153b5f5dc",
   "metadata": {},
   "source": [
    "The data are all scaled from -1 (minimum) to 1.0 (maximum).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6a05a-6747-4cf9-8bfc-3c8aba94881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, :-1].min().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44fd25-8ea1-4848-9d92-1b3782072ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, :-1].max().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c44b8c-70b4-4f2f-be9f-1c5b9b727b05",
   "metadata": {},
   "source": [
    "Examine the breakdown of activities; they are relatively balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e4dfb-3284-4cc2-a202-ed63736dfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991b67d-1668-49f4-ad08-e9231df1f5fd",
   "metadata": {},
   "source": [
    "Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n",
    "Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141d836-9798-4cb0-9b7c-bc298b094dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Activity'] = le.fit_transform(data.Activity)\n",
    "data['Activity'].sample(5)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff9222-961e-4570-9a2f-e4970d6ef226",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Calculate the correlations between the dependent variables.\n",
    "* Create a histogram of the correlation values.\n",
    "* Identify those that are most correlated (either positively or negatively).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d9513-67d6-405f-ad9f-8c075af15dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Calculate the correlation values\n",
    "feature_cols = data.columns[:-1]\n",
    "corr_values = data[feature_cols].corr()\n",
    "\n",
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "\n",
    "# Make the unused values NaNs\n",
    "for coord in zip(*tril_index):\n",
    "    corr_values.iloc[coord[0], coord[1]] = np.NaN\n",
    "    \n",
    "# Stack the data and convert to a data frame\n",
    "corr_values = (corr_values\n",
    "               .stack()\n",
    "               .to_frame()\n",
    "               .reset_index()\n",
    "               .rename(columns={'level_0':'feature1',\n",
    "                                'level_1':'feature2',\n",
    "                                0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae03dc-63d4-4b12-b1e9-e9a3f54ab44e",
   "metadata": {},
   "source": [
    "A histogram of the absolute value correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5b722-3020-4452-ada2-57e3981996d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e45f0-d5b4-420b-abcb-811565ee6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f8297-03fa-48b3-a9c0-8d68cfeeccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most highly correlated values\n",
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9081cdc-84a8-453e-9fb5-f96beafa4695",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n",
    "* Regardless of the method used to split the data, compare the ratio of classes in both the train and test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72503e40-f74e-43d0-99f7-da9cc2f3e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n",
    "                                          test_size=0.3, \n",
    "                                          random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n",
    "\n",
    "# Create the dataframes\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'Activity']\n",
    "\n",
    "X_test  = data.loc[test_idx, feature_cols]\n",
    "y_test  = data.loc[test_idx, 'Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f51d68-3427-4c99-ae2f-02ce679ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202de9a0-393c-451e-823e-feb46e9b82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52ae3a-7db6-4649-8751-34d547459b07",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n",
    "* Using cross validation to determine the hyperparameters and fit models using L1 and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a66223-4449-4f9c-a1cd-2cb13446ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standard logistic regression\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c91c4d-c703-469c-8508-febc2617df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# L1 regularized logistic regression\n",
    "lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976605e2-0ec6-4075-a724-d813d155e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385b35d-dc8d-4362-9a88-9a46943b0794",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ede455-7a3a-46ed-a356-bf47648e4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Combine all the coefficients into a dataframe\n",
    "coefficients = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    coeffs = mod.coef_\n",
    "    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n",
    "                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n",
    "    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n",
    "\n",
    "coefficients = pd.concat(coefficients, axis=1)\n",
    "\n",
    "coefficients.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53560e-56ce-49ad-a33e-d29a5a86156f",
   "metadata": {},
   "source": [
    "Prepare six separate plots for each of the multi-class coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b63a2-d6ac-44b3-8916-f17ea681cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axList = plt.subplots(nrows=3, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(10,10)\n",
    "\n",
    "for ax in enumerate(axList):\n",
    "    loc = ax[0]\n",
    "    ax = ax[1]\n",
    "    \n",
    "    data = coefficients.xs(loc, level=1, axis=1)\n",
    "    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n",
    "    \n",
    "    if ax is axList[0]:\n",
    "        ax.legend(loc=4)\n",
    "        \n",
    "    ax.set(title='Coefficient Set '+str(loc))\n",
    "\n",
    "plt.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae8563-26f1-4a7e-a245-9b66b6867437",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Predict and store the class for each model.\n",
    "* Store the probability for the predicted class for each model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432df2d-3dcc-44c2-ad5e-1e4cab14dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Predict the class and the probability for each\n",
    "y_pred = list()\n",
    "y_prob = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n",
    "    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n",
    "    \n",
    "y_pred = pd.concat(y_pred, axis=1)\n",
    "y_prob = pd.concat(y_prob, axis=1)\n",
    "\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6a455-017f-473e-be73-62b9bc6c6920",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.head()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdadfaa4-fc3f-41b1-bad4-0ce3acccbc11",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "For each model, calculate the following error metrics: \n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F-score\n",
    "* Confusion Matrix\n",
    "\n",
    "Decide how to combine the multi-class metrics into a single value for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d191c7-5de2-4778-af1a-5a30c0f8036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for lab in coeff_labels:\n",
    "\n",
    "    # Preciision, recall, f-score from the multi-class support function\n",
    "    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n",
    "    \n",
    "    # The usual way to calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred[lab])\n",
    "    \n",
    "    # ROC-AUC scores can be calculated by binarizing the data\n",
    "    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n",
    "              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n",
    "              average='weighted')\n",
    "    \n",
    "    # Last, the confusion matrix\n",
    "    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n",
    "    \n",
    "    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                              'fscore':fscore, 'accuracy':accuracy,\n",
    "                              'auc':auc}, \n",
    "                             name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2aea97-72e6-4293-981c-5d833a884c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e91083-855d-46af-9b44-1137d3720f61",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "* Display or plot the confusion matrix for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a6264-0f21-4da2-9520-e913b3101c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "fig, axList = plt.subplots(nrows=2, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "axList[-1].axis('off')\n",
    "\n",
    "for ax,lab in zip(axList[:-1], coeff_labels):\n",
    "    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n",
    "    ax.set(title=lab);\n",
    "    \n",
    "plt.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989f4da-f87c-4d3e-bb01-09c5f5e8e6a9",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "5595500a1b20d83ab72984280f4996e5d926d4c49321ec6e080dd624afd105cc"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
